{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Required Packages and checking version of TensorFlow library"
      ],
      "metadata": {
        "id": "iSndATONu8-T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "uM2ru7M0e00l",
        "outputId": "e0f5e582-bb83-4ccb-b50a-76775c549c59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Importing Required Packages\n",
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import tensorflow as tf\n",
        "plt.style.use('dark_background')\n",
        "%matplotlib inline\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting precision of printing\n",
        "np.set_printoptions(precision=2)"
      ],
      "metadata": {
        "id": "uqq-uiUJvLmV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Artificial Data i.e. Data matrix (X) and output class labels (y)\n",
        "* One-hot encoding of  the output class labels y  is done using tensorflow library"
      ],
      "metadata": {
        "id": "Q1b-FgBKwsWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Consider 10 samples, 5 features per sample and 3 output classes\n",
        "num_samples = 10\n",
        "num_features = 5\n",
        "num_labels = 3\n",
        "\n",
        "# Data matrix (each column = single sample)\n",
        "X = np.random.choice(np.arange(3, 10), size = (num_features, num_samples), replace = True)\n",
        "\n",
        "# Output Class labels\n",
        "y = np.random.choice([0, 1, 2], size = num_samples, replace = True)\n",
        "\n",
        "print('X = ')\n",
        "print(X)\n",
        "print('------')\n",
        "print('y = ')\n",
        "print(y)\n",
        "print('------')\n",
        "\n",
        "# One-hot encode class labels\n",
        "y = tf.keras.utils.to_categorical(y).T\n",
        "print(y)\n",
        "print('------')\n",
        "print('Shape of Data Matrix:', X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9AAf23pwm0I",
        "outputId": "5d002f54-4565-4e34-b3c6-0ddd35bb1bcb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X = \n",
            "[[6 8 4 4 6 3 9 5 6 5]\n",
            " [4 4 7 6 7 4 6 3 5 8]\n",
            " [4 5 5 4 7 7 6 9 5 7]\n",
            " [5 6 3 9 6 5 5 7 8 9]\n",
            " [5 6 8 9 5 6 9 4 8 8]]\n",
            "------\n",
            "y = \n",
            "[1 2 0 0 0 0 2 2 1 0]\n",
            "------\n",
            "[[0. 0. 1. 1. 1. 1. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 1. 1. 0. 0.]]\n",
            "------\n",
            "Shape of Data Matrix: (5, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generic Layer Class with Forward and Backward method"
      ],
      "metadata": {
        "id": "dgVYfq3Ex-UT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "  def __init__(self): # This is a constructor\n",
        "    self.input = None # Attribute 1 for the layer\n",
        "    self.output = None # Attribute 2 for the layer\n",
        "\n",
        "  def forward(self, input):\n",
        "    pass\n",
        "\n",
        "  def backward(self, output_gradient, learning_rate):\n",
        "    pass"
      ],
      "metadata": {
        "id": "zXOWrFo9xYM3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the Dense Layer class with Forward and Backward Method"
      ],
      "metadata": {
        "id": "YX7cPrRG3PNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dense(Layer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.empty((output_size, input_size+1), dtype = np.float64) # Bias Trick\n",
        "        self.weights[:, :-1] = 0.01 * np.random.randn(output_size, input_size) # Generating weights part of matrix W\n",
        "        self.weights[:, -1] = 0.01 # Set all bias values to the same nonzero constant\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = np.append(input, 1) # Adding Bias feature 1\n",
        "        self.output = np.dot(self.weights, self.input) # As output of Dense layer is Z = W.X\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        weights_gradient = np.dot(output_gradient.reshape(-1, 1), self.input.reshape(-1, 1).T)\n",
        "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
        "        self.weights = self.weights + learning_rate * (-weights_gradient)\n",
        "        return input_gradient"
      ],
      "metadata": {
        "id": "EzInXF03y_WJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the SoftMax Activation Layer with Forward and Backward Method"
      ],
      "metadata": {
        "id": "XNpGjQX96q9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax(Layer):\n",
        "  def forward(self, input):\n",
        "    self.output = tf.nn.softmax(input).numpy()\n",
        "\n",
        "  def backward(self, output_gradient, learning_rate = None):\n",
        "    return(np.dot((np.identity(np.size(self.output))-self.output.T) * self.output, output_gradient))"
      ],
      "metadata": {
        "id": "1Lt5bnKe6wzC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the Loss function and its gradient\n",
        "* Here, consider Categorical Cross Entropy Loss (CCE)"
      ],
      "metadata": {
        "id": "Uo0S0QDI7lCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cce(y, yhat):\n",
        "  return(-np.sum(y*np.log(yhat)))\n",
        "  #return(np.dot(-y, np.log(yhat)))\n",
        "\n",
        "def cce_gradient(y, yhat):\n",
        "  return(-y/yhat)\n",
        "\n",
        "# TensorFlow in-built function for categorical crossentropy loss\n",
        "#cce = tf.keras.losses.CategoricalCrossentropy()"
      ],
      "metadata": {
        "id": "mtGMpQ8J7c8E"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward and backward propagation for the 0th sample"
      ],
      "metadata": {
        "id": "qEH0sP0EOUEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-02 # Define Learning Rate\n",
        "\n",
        "# Define the neural network architecture\n",
        "\n",
        "dlayer = Dense(num_features, num_labels) # define the dense layer\n",
        "#print(dlayer.weights)\n",
        "softmax = Softmax() # define the softmax activation layer\n",
        "print('Weights for Dense Layer W is')\n",
        "print(dlayer.weights) # The last column is always 0.01 as this is the bias\n",
        "print('---------------------------------------------------------')\n",
        "\n",
        "# Forward propagation starts here\n",
        "\n",
        "dlayer.forward(X[:, 0]) # forward prop for the 0th sample\n",
        "softmax.forward(dlayer.output) # The input of Softmax actn layer is Z, which is the output of dlayer\n",
        "loss = cce(y[:, 0], softmax.output) # softmax.output gives the predicted probabities vector yhat\n",
        "print('The Calculated Loss using Forward Propagation Method L is')\n",
        "print(loss)\n",
        "print('---------------------------------------------------------')\n",
        "\n",
        "# Backward propagation starts here\n",
        "\n",
        "grad = cce_gradient(y[:, 0], softmax.output)\n",
        "grad = softmax.backward(grad)\n",
        "grad = dlayer.backward(grad, learning_rate)\n",
        "print('After Updating the Updated Weights are given as')\n",
        "print(dlayer.weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI5e2sQQBUwO",
        "outputId": "9a4a3eb8-46ba-4a4f-c7a5-5c828c17f793"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights for Dense Layer W is\n",
            "[[-0.   -0.01  0.   -0.01  0.01  0.01]\n",
            " [ 0.01 -0.02 -0.    0.01 -0.01  0.01]\n",
            " [ 0.   -0.   -0.01  0.01  0.01  0.01]]\n",
            "---------------------------------------------------------\n",
            "The Calculated Loss using Forward Propagation Method L is\n",
            "1.1391484979535902\n",
            "---------------------------------------------------------\n",
            "After Updating the Updated Weights are given as\n",
            "[[-0.02 -0.02 -0.01 -0.03 -0.    0.01]\n",
            " [ 0.05  0.01  0.03  0.04  0.03  0.02]\n",
            " [-0.02 -0.01 -0.02 -0.   -0.    0.01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward and Backward Propagation for All Samples\n",
        "* Consider the batch size 1"
      ],
      "metadata": {
        "id": "Z-yrTqNUSIjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Steps: run over each sample, calculate loss, gradient of loss, and update weights.\n",
        "\n",
        "learning_rate = 1e-02 # Learning Rate\n",
        "\n",
        "# Define the neural network architecture\n",
        "dlayer = Dense(num_features, num_labels) # define the dense layer\n",
        "\n",
        "#print(dlayer.weights)\n",
        "\n",
        "softmax = Softmax() # define the softmax activation layer\n",
        "\n",
        "loss = 0.0\n",
        "\n",
        "for i in range(X.shape[1]):\n",
        "  # Forward propagation starts here\n",
        "  dlayer.forward(X[:, i]) # forward prop for the 0th sample\n",
        "  softmax.forward(dlayer.output)\n",
        "  loss = cce(y[:, i], softmax.output)\n",
        "\n",
        "  # Backward propagation starts here\n",
        "  grad = cce_gradient(y[:, i], softmax.output)\n",
        "  grad = softmax.backward(grad)\n",
        "  grad = dlayer.backward(grad, learning_rate)\n",
        "\n",
        "  print('Sample %d, loss = %f'%(i, loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xt8R_ci5O4we",
        "outputId": "dd7b37ba-e628-42df-bbf5-37900208a0a7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0, loss = 1.207796\n",
            "Sample 1, loss = 1.713001\n",
            "Sample 2, loss = 2.086332\n",
            "Sample 3, loss = 1.012696\n",
            "Sample 4, loss = 0.225098\n",
            "Sample 5, loss = 0.067376\n",
            "Sample 6, loss = 5.987081\n",
            "Sample 7, loss = 2.967725\n",
            "Sample 8, loss = 6.293487\n",
            "Sample 9, loss = 0.130480\n"
          ]
        }
      ]
    }
  ]
}