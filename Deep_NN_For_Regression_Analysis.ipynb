{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "914eAJzvufap"
      },
      "source": [
        "### Load libraries and Checking the TensorFlow Library Vesrion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5dEgRpy3952M",
        "outputId": "a4254a17-f5da-4bd8-e447-1bd791c084c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "plt.style.use('dark_background')\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v2NCtj6ulc1"
      },
      "source": [
        "### Setting the Printing Precisions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "G9W_1_v_6yq7"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbaFOIn_CDuN"
      },
      "source": [
        "\n",
        "### Mount Google Drive in Colab Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02xk1yt7CE1D",
        "outputId": "46e67241-006a-4301-c007-7657e61ba282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "## Mount Google drive folder if running in Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16BpVeIWIOks"
      },
      "source": [
        "### Load diabetes data , named as diabetes_regression.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "E5kaKFKSIQgu",
        "outputId": "0b110f2a-feee-4f0c-e997-b47f1d505b4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diabetes dataset\n",
            "-----------\n",
            "Initial number of samples = 442\n",
            "Initial number of features = 11\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   AGE  GENDER    BMILEVEL     BP   S1     S2    S3   S4      S5  S6    Y\n",
              "0   59       2   unhealthy  101.0  157   93.2  38.0  4.0  4.8598  87  151\n",
              "1   48       1     healthy   87.0  183  103.2  70.0  3.0  3.8918  69   75\n",
              "2   72       2   unhealthy   93.0  156   93.6  41.0  4.0  4.6728  85  141\n",
              "3   24       1  overweight   84.0  198  131.4  40.0  5.0  4.8903  89  206\n",
              "4   50       1     healthy  101.0  192  125.4  52.0  4.0  4.2905  80  135"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ec81e40e-ebef-4566-adc8-e43ce015f6fe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AGE</th>\n",
              "      <th>GENDER</th>\n",
              "      <th>BMILEVEL</th>\n",
              "      <th>BP</th>\n",
              "      <th>S1</th>\n",
              "      <th>S2</th>\n",
              "      <th>S3</th>\n",
              "      <th>S4</th>\n",
              "      <th>S5</th>\n",
              "      <th>S6</th>\n",
              "      <th>Y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>59</td>\n",
              "      <td>2</td>\n",
              "      <td>unhealthy</td>\n",
              "      <td>101.0</td>\n",
              "      <td>157</td>\n",
              "      <td>93.2</td>\n",
              "      <td>38.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.8598</td>\n",
              "      <td>87</td>\n",
              "      <td>151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>48</td>\n",
              "      <td>1</td>\n",
              "      <td>healthy</td>\n",
              "      <td>87.0</td>\n",
              "      <td>183</td>\n",
              "      <td>103.2</td>\n",
              "      <td>70.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.8918</td>\n",
              "      <td>69</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>72</td>\n",
              "      <td>2</td>\n",
              "      <td>unhealthy</td>\n",
              "      <td>93.0</td>\n",
              "      <td>156</td>\n",
              "      <td>93.6</td>\n",
              "      <td>41.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.6728</td>\n",
              "      <td>85</td>\n",
              "      <td>141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>overweight</td>\n",
              "      <td>84.0</td>\n",
              "      <td>198</td>\n",
              "      <td>131.4</td>\n",
              "      <td>40.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.8903</td>\n",
              "      <td>89</td>\n",
              "      <td>206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "      <td>healthy</td>\n",
              "      <td>101.0</td>\n",
              "      <td>192</td>\n",
              "      <td>125.4</td>\n",
              "      <td>52.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.2905</td>\n",
              "      <td>80</td>\n",
              "      <td>135</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec81e40e-ebef-4566-adc8-e43ce015f6fe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ec81e40e-ebef-4566-adc8-e43ce015f6fe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ec81e40e-ebef-4566-adc8-e43ce015f6fe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-19285622-9420-49da-a523-b68541d8fceb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-19285622-9420-49da-a523-b68541d8fceb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-19285622-9420-49da-a523-b68541d8fceb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 442,\n  \"fields\": [\n    {\n      \"column\": \"AGE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13,\n        \"min\": 19,\n        \"max\": 79,\n        \"num_unique_values\": 58,\n        \"samples\": [\n          59,\n          23,\n          54\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GENDER\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BMILEVEL\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"healthy\",\n          \"underweight\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13.831283419782999,\n        \"min\": 62.0,\n        \"max\": 133.0,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          104.33,\n          102.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"S1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 34,\n        \"min\": 97,\n        \"max\": 301,\n        \"num_unique_values\": 141,\n        \"samples\": [\n          219,\n          250\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"S2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30.413080969276546,\n        \"min\": 41.6,\n        \"max\": 242.4,\n        \"num_unique_values\": 302,\n        \"samples\": [\n          162.8,\n          160.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"S3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.934202154863327,\n        \"min\": 22.0,\n        \"max\": 99.0,\n        \"num_unique_values\": 63,\n        \"samples\": [\n          75.0,\n          93.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"S4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2904498966082774,\n        \"min\": 2.0,\n        \"max\": 9.09,\n        \"num_unique_values\": 66,\n        \"samples\": [\n          6.42,\n          3.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"S5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5223905610694907,\n        \"min\": 3.2581,\n        \"max\": 6.107,\n        \"num_unique_values\": 184,\n        \"samples\": [\n          3.8501,\n          4.4067\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"S6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 58,\n        \"max\": 124,\n        \"num_unique_values\": 56,\n        \"samples\": [\n          87,\n          68\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Y\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 77,\n        \"min\": 25,\n        \"max\": 346,\n        \"num_unique_values\": 214,\n        \"samples\": [\n          310,\n          140\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "## Load diabetes dataset, the data set works as data matrix X\n",
        "df = pd.read_csv('/content/drive/MyDrive/Project_Data_Sets/diabetes_regression.csv')\n",
        "\n",
        "print('Diabetes dataset')\n",
        "print('-----------')\n",
        "print('Initial number of samples = %d'%(df.shape[0])) # Samples\n",
        "print('Initial number of features = %d\\n'%(df.shape[1])) # Features\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r1wv2EuxywH"
      },
      "source": [
        "### Create lists of ordinal, categorical, and continuous features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJE5ehBOClXW",
        "outputId": "5c326be4-5146-4051-d28c-1c375784e1dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['GENDER', 'BMILEVEL']\n",
            "['AGE', 'BP', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'Y']\n"
          ]
        }
      ],
      "source": [
        "categorical_features = ['GENDER', 'BMILEVEL']\n",
        "continuous_features = df.drop(categorical_features, axis = 1).columns.tolist()\n",
        "print(categorical_features)\n",
        "print(continuous_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3NyP_EoDG1i"
      },
      "source": [
        "### Assign 'category' datatype to categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dVFfOBlDJ5n",
        "outputId": "b2039229-bd55-49b8-9b13-1dbb9ece4527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AGE           int64\n",
            "GENDER        int64\n",
            "BMILEVEL     object\n",
            "BP          float64\n",
            "S1            int64\n",
            "S2          float64\n",
            "S3          float64\n",
            "S4          float64\n",
            "S5          float64\n",
            "S6            int64\n",
            "Y             int64\n",
            "dtype: object\n",
            "----\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AGE            int64\n",
              "GENDER      category\n",
              "BMILEVEL    category\n",
              "BP           float64\n",
              "S1             int64\n",
              "S2           float64\n",
              "S3           float64\n",
              "S4           float64\n",
              "S5           float64\n",
              "S6             int64\n",
              "Y              int64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "## Assign 'category' datatype to ordinal and categorical columns\n",
        "print(df.dtypes)\n",
        "df[categorical_features] = df[categorical_features].astype('category')\n",
        "print('----')\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m95YNt2eDUJ8"
      },
      "source": [
        "### Remove the target variable column from the list of continuous features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XuQGzpefDUqE"
      },
      "outputs": [],
      "source": [
        "## Remove the target variable column from the list of continuous features\n",
        "continuous_features.remove('Y')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0lJslQQzcbh"
      },
      "source": [
        "### Train and test split of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tn-qjRocE8Lj",
        "outputId": "d8dde07d-24fb-42d9-ddb9-d6f55dbce20d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diabetes data set\n",
            "---------------------\n",
            "Number of training samples = 353\n",
            "Number of features = 10\n"
          ]
        }
      ],
      "source": [
        "X = df.drop('Y', axis = 1)\n",
        "y = df['Y']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
        "\n",
        "num_features = X_train.shape[1]\n",
        "num_samples = X_train.shape[0]\n",
        "\n",
        "print('Diabetes data set')\n",
        "print('---------------------')\n",
        "print('Number of training samples = %d'%(num_samples))\n",
        "print('Number of features = %d'%(num_features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKqSu9e7G-fh"
      },
      "source": [
        "### Build pipeline for categorical and continuous features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FKAqkz1GHGvm"
      },
      "outputs": [],
      "source": [
        "## Build pipeline for categorical and continuous features\n",
        "\n",
        "# Pipeline object for categorical (features\n",
        "categorical_transformer = Pipeline(steps = [('onehotenc', OneHotEncoder(handle_unknown = 'ignore'))])\n",
        "\n",
        "# Pipeline object for continuous features\n",
        "continuous_transformer = Pipeline(steps = [('scaler', RobustScaler())])\n",
        "\n",
        "# Create a preprocessor object for all features\n",
        "preprocessor = ColumnTransformer(transformers = [('continuous', continuous_transformer, continuous_features),\n",
        "                                                 ('categorical', categorical_transformer, categorical_features)\n",
        "                                                ],\n",
        "                                 remainder = 'passthrough'\n",
        "                                 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUWtfRseHv2O"
      },
      "source": [
        "### Fit and transform train data using preprocessor followed by transforming test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klmniK8oGBpz",
        "outputId": "7b54b3a4-8dfb-41fa-946c-b3b271c7b278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AGE              47\n",
            "GENDER            2\n",
            "BMILEVEL    healthy\n",
            "BP             75.0\n",
            "S1              225\n",
            "S2            166.0\n",
            "S3             42.0\n",
            "S4              5.0\n",
            "S5           4.4427\n",
            "S6              102\n",
            "Name: 438, dtype: object\n",
            "------\n",
            "[-0.16 -0.9   0.89  1.42 -0.35  0.5  -0.25  0.67  0.    1.    1.    0.\n",
            "  0.    0.  ]\n"
          ]
        }
      ],
      "source": [
        "print(X_train.iloc[0, :])\n",
        "print('------')\n",
        "X_train_transformed = preprocessor.fit_transform(X_train)\n",
        "print(X_train_transformed[0, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit and transform train data using preprocessor"
      ],
      "metadata": {
        "id": "S8V9InnKHkp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_transformed = preprocessor.fit_transform(X_train).T\n",
        "# Update number of features\n",
        "num_features = X_train_transformed.shape[0]\n",
        "# Transform training data using preprocessor\n",
        "X_test_transformed = preprocessor.transform(X_test).T\n",
        "# Convert Y_train and Y_test to numpy arrays\n",
        "type(Y_train) # = Y_train.to_numpy()\n",
        "type(Y_test) # = Y_test.to_numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qe7_dM-HtxC",
        "outputId": "9cf870a6-1f1f-435d-af54-dcef3472ab4a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYn_hOccDa3M"
      },
      "source": [
        "### A generic layer class with forward and backward methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "N4pKUhCyMrWm"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "    self.output = None\n",
        "\n",
        "  def forward(self, input):\n",
        "    pass\n",
        "\n",
        "  def backward(self, output_gradient, learning_rate):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMt81Faf9-bf"
      },
      "source": [
        "### Mean squared error (MSE) loss and its gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hdXSGW2s7zKd"
      },
      "outputs": [],
      "source": [
        "## Define the loss function and its gradient\n",
        "def mse(Y, Yhat):\n",
        "  return(np.mean(0.5*(Y - Yhat)**2))\n",
        "  #TensorFlow in-built function for mean squared error loss\n",
        "  #mse = tf.keras.losses.MeanSquaredError()\n",
        "  #mse(Y, Yhat).numpy()\n",
        "\n",
        "def mse_gradient(Y, Yhat):\n",
        "  return(Yhat - Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmcNJTjS-BaW"
      },
      "source": [
        "### Generic activation layer class with Forward and Backward method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "C21FcWIEwGCN"
      },
      "outputs": [],
      "source": [
        "class Activation(Layer):\n",
        "    def __init__(self, activation, activation_gradient):\n",
        "        self.activation = activation\n",
        "        self.activation_gradient = activation_gradient\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        self.output = self.activation(self.input)\n",
        "        return(self.output)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate = None):\n",
        "        return(output_gradient[:-1, :] * self.activation_gradient(self.input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JheGWSoKxYWu"
      },
      "source": [
        "### Example of Specific activation layer classes and their gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PQ5ybz_Yxbef"
      },
      "outputs": [],
      "source": [
        "class Sigmoid(Activation):\n",
        "    def __init__(self):\n",
        "        def sigmoid(z):\n",
        "            return 1 / (1 + np.exp(-z))\n",
        "\n",
        "        def sigmoid_gradient(z):\n",
        "            a = sigmoid(z)\n",
        "            return a * (1 - a)\n",
        "\n",
        "        super().__init__(sigmoid, sigmoid_gradient)\n",
        "\n",
        "class Tanh(Activation):\n",
        "    def __init__(self):\n",
        "        def tanh(z):\n",
        "            return np.tanh(z)\n",
        "\n",
        "        def tanh_gradient(z):\n",
        "            a = np.tanh(z)\n",
        "            return 1 - a**2\n",
        "\n",
        "        super().__init__(tanh, tanh_gradient)\n",
        "\n",
        "class ReLU(Activation):\n",
        "    def __init__(self):\n",
        "        def relu(z):\n",
        "            return z * (z > 0)\n",
        "\n",
        "        def relu_gradient(z):\n",
        "            return 1. * (z > 0)\n",
        "\n",
        "        super().__init__(relu, relu_gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKnqi7rf-MBn"
      },
      "source": [
        "### Dense layer class and its Forward and Backward Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8ctXhZYCTmHK"
      },
      "outputs": [],
      "source": [
        "## Dense layer class\n",
        "class Dense(Layer):\n",
        "    def __init__(self, input_size, output_size, reg_strength):\n",
        "        self.weights = 0.01*np.random.randn(output_size, input_size+1) # bias trick\n",
        "        self.weights[:, -1] = 0.01 # set all bias values to the same nonzero constant\n",
        "        self.reg_strength = reg_strength\n",
        "        self.reg_loss = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = np.vstack([input, np.ones((1, input.shape[1]))]) # bias trick\n",
        "        self.output= np.dot(self.weights, self.input)\n",
        "        # Calculate regularization loss\n",
        "        self.reg_loss = self.reg_strength * np.sum(self.weights[:, :-1] * self.weights[:, :-1])\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        ## Following is the inefficient way of calculating the backward gradient\n",
        "        #weights_gradient = np.zeros((self.output.shape[0], self.input.shape[0]), dtype = np.float64)\n",
        "        #for b in range(output_gradient.shape[1]):\n",
        "        #  weights_gradient += np.dot(output_gradient[:, b].reshape(-1, 1), self.input[:, b].reshape(-1, 1).T)\n",
        "        #weights_gradient = (1/output_gradient.shape[1])*weights_gradient\n",
        "\n",
        "        ## Following is the efficient way of calculating the weights gradient w.r.t. data\n",
        "        weights_gradient = (1/output_gradient.shape[1])*np.dot(np.atleast_2d(output_gradient), np.atleast_2d(self.input).T)\n",
        "        # Add the regularization gradient here\n",
        "        weights_gradient += 2 * self.reg_strength * np.hstack([self.weights[:, :-1], np.zeros((self.weights.shape[0], 1))])\n",
        "\n",
        "\n",
        "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
        "        self.weights = self.weights + learning_rate * (-weights_gradient)\n",
        "\n",
        "        return(input_gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W1howeOJegI"
      },
      "source": [
        "### Function to generate sample indices for batch processing according to batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MHyjEf22IRpc"
      },
      "outputs": [],
      "source": [
        "## Function to generate sample indices for batch processing according to batch size\n",
        "def generate_batch_indices(num_samples, batch_size):\n",
        "  # Reorder sample indices\n",
        "  reordered_sample_indices = np.random.choice(num_samples, num_samples, replace = False)\n",
        "  # Generate batch indices for batch processing\n",
        "  batch_indices = np.split(reordered_sample_indices, np.arange(batch_size, len(reordered_sample_indices), batch_size))\n",
        "  return(batch_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI_Gms9fJqbs"
      },
      "source": [
        "### Train the 2-layer neural network (8 nodes followed by 1 node) using batch training with batch size = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LGIzrN-rPuI4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28ca2b5d-a5b7-47a8-caad-5fc0872ec1c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train loss = 14191.800748, test loss = 12860.862244\n",
            "Epoch 2: train loss = 7711.000657, test loss = 2225.091247\n",
            "Epoch 3: train loss = 1651.970071, test loss = 1768.066679\n",
            "Epoch 4: train loss = 1595.410520, test loss = 1922.745477\n",
            "Epoch 5: train loss = 1650.514558, test loss = 1866.949418\n",
            "Epoch 6: train loss = 1900.150536, test loss = 3658.987223\n",
            "Epoch 7: train loss = 1978.437495, test loss = 2131.567737\n",
            "Epoch 8: train loss = 1730.245174, test loss = 1929.802990\n",
            "Epoch 9: train loss = 1630.153278, test loss = 1785.061127\n",
            "Epoch 10: train loss = 1629.866709, test loss = 2467.957939\n",
            "Epoch 11: train loss = 1807.793095, test loss = 2307.799165\n",
            "Epoch 12: train loss = 1623.743306, test loss = 1755.660811\n",
            "Epoch 13: train loss = 1669.727464, test loss = 2212.514324\n",
            "Epoch 14: train loss = 1621.173213, test loss = 1877.797241\n",
            "Epoch 15: train loss = 1577.824742, test loss = 1886.821763\n",
            "Epoch 16: train loss = 1647.555552, test loss = 2002.068392\n",
            "Epoch 17: train loss = 1683.620537, test loss = 1956.550391\n",
            "Epoch 18: train loss = 1630.624199, test loss = 1804.979566\n",
            "Epoch 19: train loss = 1584.311588, test loss = 1752.673400\n",
            "Epoch 20: train loss = 1680.934309, test loss = 2623.825705\n",
            "Epoch 21: train loss = 1807.349381, test loss = 2344.493926\n",
            "Epoch 22: train loss = 1776.608669, test loss = 2091.962356\n",
            "Epoch 23: train loss = 1620.074726, test loss = 1774.182109\n",
            "Epoch 24: train loss = 1605.165140, test loss = 1996.713782\n",
            "Epoch 25: train loss = 1608.076093, test loss = 2075.044822\n",
            "Epoch 26: train loss = 1746.167091, test loss = 2601.448212\n",
            "Epoch 27: train loss = 1663.684341, test loss = 1809.103636\n",
            "Epoch 28: train loss = 1626.127337, test loss = 2089.478943\n",
            "Epoch 29: train loss = 1866.384698, test loss = 3252.207254\n",
            "Epoch 30: train loss = 1705.863167, test loss = 1802.542417\n",
            "Epoch 31: train loss = 1704.861683, test loss = 2366.778171\n",
            "Epoch 32: train loss = 1760.251686, test loss = 2543.547953\n",
            "Epoch 33: train loss = 1655.695639, test loss = 1792.248835\n",
            "Epoch 34: train loss = 1963.595265, test loss = 3989.866963\n",
            "Epoch 35: train loss = 1836.153256, test loss = 1746.433994\n",
            "Epoch 36: train loss = 1590.990633, test loss = 1769.181529\n",
            "Epoch 37: train loss = 1687.352674, test loss = 2920.220484\n",
            "Epoch 38: train loss = 1717.536964, test loss = 1729.114311\n",
            "Epoch 39: train loss = 1659.005121, test loss = 2170.033437\n",
            "Epoch 40: train loss = 1601.891145, test loss = 1763.151895\n",
            "Epoch 41: train loss = 1566.918044, test loss = 1739.089098\n",
            "Epoch 42: train loss = 1555.602400, test loss = 1805.510433\n",
            "Epoch 43: train loss = 1553.762267, test loss = 1835.769958\n",
            "Epoch 44: train loss = 1577.780338, test loss = 1761.419351\n",
            "Epoch 45: train loss = 1622.474908, test loss = 1810.817235\n",
            "Epoch 46: train loss = 1626.911255, test loss = 2235.521197\n",
            "Epoch 47: train loss = 1641.291587, test loss = 2064.110099\n",
            "Epoch 48: train loss = 1622.557360, test loss = 1765.904049\n",
            "Epoch 49: train loss = 1570.157795, test loss = 2005.734820\n",
            "Epoch 50: train loss = 1588.210432, test loss = 1947.316161\n",
            "Epoch 51: train loss = 1565.821727, test loss = 1752.274148\n",
            "Epoch 52: train loss = 1586.150106, test loss = 1992.919043\n",
            "Epoch 53: train loss = 1894.380517, test loss = 3536.881704\n",
            "Epoch 54: train loss = 1836.487995, test loss = 1826.792671\n",
            "Epoch 55: train loss = 1567.506202, test loss = 1860.576126\n",
            "Epoch 56: train loss = 1672.434160, test loss = 2043.973429\n",
            "Epoch 57: train loss = 1698.579791, test loss = 2347.572444\n",
            "Epoch 58: train loss = 1632.150608, test loss = 2025.517494\n",
            "Epoch 59: train loss = 1682.179565, test loss = 2487.261533\n",
            "Epoch 60: train loss = 1643.729395, test loss = 1946.749757\n",
            "Epoch 61: train loss = 1562.097684, test loss = 1833.361026\n",
            "Epoch 62: train loss = 1569.252601, test loss = 2032.098950\n",
            "Epoch 63: train loss = 1604.818161, test loss = 2106.272212\n",
            "Epoch 64: train loss = 1951.768599, test loss = 4044.530305\n",
            "Epoch 65: train loss = 1870.325569, test loss = 1873.111574\n",
            "Epoch 66: train loss = 1637.592705, test loss = 2574.183764\n",
            "Epoch 67: train loss = 1771.640311, test loss = 2207.424877\n",
            "Epoch 68: train loss = 1779.204873, test loss = 2410.996607\n",
            "Epoch 69: train loss = 1634.310127, test loss = 1730.946472\n",
            "Epoch 70: train loss = 1551.912809, test loss = 1748.113865\n",
            "Epoch 71: train loss = 1623.406907, test loss = 1926.181513\n",
            "Epoch 72: train loss = 1596.020768, test loss = 1810.050801\n",
            "Epoch 73: train loss = 1548.867860, test loss = 2010.918814\n",
            "Epoch 74: train loss = 1679.784438, test loss = 2117.981370\n",
            "Epoch 75: train loss = 1581.757875, test loss = 1751.789019\n",
            "Epoch 76: train loss = 1556.949496, test loss = 1787.127968\n",
            "Epoch 77: train loss = 1573.768358, test loss = 1911.192364\n",
            "Epoch 78: train loss = 1607.766753, test loss = 2140.851542\n",
            "Epoch 79: train loss = 1584.058007, test loss = 1743.438122\n",
            "Epoch 80: train loss = 1651.702883, test loss = 2701.281158\n",
            "Epoch 81: train loss = 1652.551526, test loss = 1812.187229\n",
            "Epoch 82: train loss = 1551.826997, test loss = 1908.686380\n",
            "Epoch 83: train loss = 1554.388682, test loss = 1737.451682\n",
            "Epoch 84: train loss = 1611.663411, test loss = 2063.789921\n",
            "Epoch 85: train loss = 1684.318382, test loss = 2215.865954\n",
            "Epoch 86: train loss = 1738.126058, test loss = 2845.556175\n",
            "Epoch 87: train loss = 1978.810850, test loss = 3327.413423\n",
            "Epoch 88: train loss = 1697.971458, test loss = 1753.107243\n",
            "Epoch 89: train loss = 1708.980265, test loss = 2379.989023\n",
            "Epoch 90: train loss = 1660.146860, test loss = 2003.189982\n",
            "Epoch 91: train loss = 1653.733880, test loss = 2006.214081\n",
            "Epoch 92: train loss = 1596.978653, test loss = 1733.254023\n",
            "Epoch 93: train loss = 1559.385972, test loss = 1957.169191\n",
            "Epoch 94: train loss = 1571.816196, test loss = 1904.487525\n",
            "Epoch 95: train loss = 1545.066146, test loss = 1720.558190\n",
            "Epoch 96: train loss = 1617.540016, test loss = 1890.514229\n",
            "Epoch 97: train loss = 1572.759750, test loss = 1903.184904\n",
            "Epoch 98: train loss = 1543.890618, test loss = 1734.099034\n",
            "Epoch 99: train loss = 1597.688564, test loss = 1991.545526\n",
            "Epoch 100: train loss = 1588.246583, test loss = 1998.322518\n",
            "Epoch 101: train loss = 1603.533592, test loss = 1803.567542\n",
            "Epoch 102: train loss = 1542.359803, test loss = 1726.977921\n",
            "Epoch 103: train loss = 1641.756669, test loss = 2157.874707\n",
            "Epoch 104: train loss = 1618.969046, test loss = 1780.021341\n",
            "Epoch 105: train loss = 1586.262015, test loss = 2009.264220\n",
            "Epoch 106: train loss = 1571.798490, test loss = 1848.494569\n",
            "Epoch 107: train loss = 1580.819052, test loss = 1774.384656\n",
            "Epoch 108: train loss = 1565.604725, test loss = 1737.944931\n",
            "Epoch 109: train loss = 1589.539209, test loss = 2239.277776\n",
            "Epoch 110: train loss = 1604.421404, test loss = 2080.690843\n",
            "Epoch 111: train loss = 1734.061432, test loss = 2429.634533\n",
            "Epoch 112: train loss = 1740.167468, test loss = 2000.627067\n",
            "Epoch 113: train loss = 1686.243929, test loss = 2312.518965\n",
            "Epoch 114: train loss = 1598.850024, test loss = 1804.752934\n",
            "Epoch 115: train loss = 1595.267928, test loss = 2137.868127\n",
            "Epoch 116: train loss = 1708.875957, test loss = 2356.575474\n",
            "Epoch 117: train loss = 1644.586278, test loss = 2765.008348\n",
            "Epoch 118: train loss = 1657.614372, test loss = 1733.472626\n",
            "Epoch 119: train loss = 1515.205989, test loss = 1766.573831\n",
            "Epoch 120: train loss = 1538.485972, test loss = 1927.216639\n",
            "Epoch 121: train loss = 1849.091735, test loss = 2802.753542\n",
            "Epoch 122: train loss = 1726.330238, test loss = 2336.270674\n",
            "Epoch 123: train loss = 1605.005051, test loss = 1788.498090\n",
            "Epoch 124: train loss = 1571.820403, test loss = 1724.182530\n",
            "Epoch 125: train loss = 1583.345425, test loss = 1797.110054\n",
            "Epoch 126: train loss = 1595.758161, test loss = 1806.130547\n",
            "Epoch 127: train loss = 1618.997626, test loss = 2107.017885\n",
            "Epoch 128: train loss = 1588.518605, test loss = 1854.002557\n",
            "Epoch 129: train loss = 1570.002333, test loss = 1844.063901\n",
            "Epoch 130: train loss = 2068.289219, test loss = 5279.505541\n",
            "Epoch 131: train loss = 2049.824039, test loss = 2401.114932\n",
            "Epoch 132: train loss = 1597.365757, test loss = 1719.529804\n",
            "Epoch 133: train loss = 1525.047006, test loss = 1768.305339\n",
            "Epoch 134: train loss = 1523.680256, test loss = 1738.396510\n",
            "Epoch 135: train loss = 1631.327039, test loss = 2146.337187\n",
            "Epoch 136: train loss = 1607.526150, test loss = 1774.560400\n",
            "Epoch 137: train loss = 1751.350410, test loss = 3127.509615\n",
            "Epoch 138: train loss = 1736.704018, test loss = 2018.641402\n",
            "Epoch 139: train loss = 1767.164140, test loss = 2013.889725\n",
            "Epoch 140: train loss = 1578.778316, test loss = 1852.243797\n",
            "Epoch 141: train loss = 1568.061260, test loss = 1868.422922\n",
            "Epoch 142: train loss = 1537.005194, test loss = 1838.049731\n",
            "Epoch 143: train loss = 1638.316009, test loss = 2016.791625\n",
            "Epoch 144: train loss = 1685.796717, test loss = 2012.421619\n",
            "Epoch 145: train loss = 1862.698476, test loss = 4130.974469\n",
            "Epoch 146: train loss = 1709.713618, test loss = 1708.439311\n",
            "Epoch 147: train loss = 1568.302004, test loss = 1822.065499\n",
            "Epoch 148: train loss = 1559.569870, test loss = 1804.336839\n",
            "Epoch 149: train loss = 1541.835375, test loss = 1746.679464\n",
            "Epoch 150: train loss = 1534.754362, test loss = 1784.531980\n",
            "Epoch 151: train loss = 1572.354239, test loss = 1821.313660\n",
            "Epoch 152: train loss = 1606.561783, test loss = 2008.122843\n",
            "Epoch 153: train loss = 1564.781324, test loss = 1753.069367\n",
            "Epoch 154: train loss = 1565.441483, test loss = 1727.235310\n",
            "Epoch 155: train loss = 1553.789407, test loss = 1721.709776\n",
            "Epoch 156: train loss = 1554.234778, test loss = 1728.187969\n",
            "Epoch 157: train loss = 1801.809327, test loss = 2828.881944\n",
            "Epoch 158: train loss = 1742.030761, test loss = 2292.559577\n",
            "Epoch 159: train loss = 1622.254947, test loss = 1950.939763\n",
            "Epoch 160: train loss = 1595.367415, test loss = 1846.383831\n",
            "Epoch 161: train loss = 1570.560858, test loss = 1952.043251\n",
            "Epoch 162: train loss = 1675.116597, test loss = 1840.752937\n",
            "Epoch 163: train loss = 1768.070088, test loss = 2358.979720\n",
            "Epoch 164: train loss = 1717.932483, test loss = 2216.820285\n",
            "Epoch 165: train loss = 1608.201318, test loss = 2048.063282\n",
            "Epoch 166: train loss = 1629.874979, test loss = 1805.479866\n",
            "Epoch 167: train loss = 1666.968482, test loss = 2936.274719\n",
            "Epoch 168: train loss = 1698.601619, test loss = 1761.687678\n",
            "Epoch 169: train loss = 1672.556617, test loss = 2507.349830\n",
            "Epoch 170: train loss = 1619.976671, test loss = 1723.476877\n",
            "Epoch 171: train loss = 1584.333503, test loss = 1765.137760\n",
            "Epoch 172: train loss = 1568.069427, test loss = 1716.932074\n",
            "Epoch 173: train loss = 1613.963731, test loss = 2206.258531\n",
            "Epoch 174: train loss = 1559.467905, test loss = 1793.155645\n",
            "Epoch 175: train loss = 1994.115620, test loss = 4438.903315\n",
            "Epoch 176: train loss = 2197.391274, test loss = 4711.525215\n",
            "Epoch 177: train loss = 1786.383232, test loss = 1740.840727\n",
            "Epoch 178: train loss = 1572.847006, test loss = 2041.763194\n",
            "Epoch 179: train loss = 1775.916385, test loss = 2995.336166\n",
            "Epoch 180: train loss = 1682.707130, test loss = 1756.892966\n",
            "Epoch 181: train loss = 1609.985741, test loss = 1887.622210\n",
            "Epoch 182: train loss = 1803.332296, test loss = 2676.734050\n",
            "Epoch 183: train loss = 1679.138787, test loss = 1751.333379\n",
            "Epoch 184: train loss = 1666.441140, test loss = 1931.384787\n",
            "Epoch 185: train loss = 1612.719713, test loss = 1757.074473\n",
            "Epoch 186: train loss = 1549.919353, test loss = 1845.887093\n",
            "Epoch 187: train loss = 1558.222517, test loss = 1755.184438\n",
            "Epoch 188: train loss = 1673.180797, test loss = 2386.216175\n",
            "Epoch 189: train loss = 1632.552856, test loss = 1989.959658\n",
            "Epoch 190: train loss = 1559.931342, test loss = 1954.858099\n",
            "Epoch 191: train loss = 1508.445942, test loss = 1763.893434\n",
            "Epoch 192: train loss = 1647.340310, test loss = 2137.241965\n",
            "Epoch 193: train loss = 1657.078730, test loss = 2468.054213\n",
            "Epoch 194: train loss = 1616.603866, test loss = 1713.851949\n",
            "Epoch 195: train loss = 1630.600671, test loss = 2411.476599\n",
            "Epoch 196: train loss = 1984.198149, test loss = 4407.684070\n",
            "Epoch 197: train loss = 1784.462199, test loss = 1769.243788\n",
            "Epoch 198: train loss = 1589.387588, test loss = 1850.806962\n",
            "Epoch 199: train loss = 1739.141620, test loss = 2320.038645\n",
            "Epoch 200: train loss = 1643.282635, test loss = 1763.002352\n",
            "Epoch 201: train loss = 1607.964621, test loss = 1817.928315\n",
            "Epoch 202: train loss = 1570.874238, test loss = 2055.755808\n",
            "Epoch 203: train loss = 1669.754357, test loss = 1940.555959\n",
            "Epoch 204: train loss = 1586.705754, test loss = 1751.514408\n",
            "Epoch 205: train loss = 1558.858773, test loss = 2837.982004\n",
            "Epoch 206: train loss = 1647.138648, test loss = 1742.080605\n",
            "Epoch 207: train loss = 1593.008655, test loss = 1865.252378\n",
            "Epoch 208: train loss = 1746.689812, test loss = 3024.890552\n",
            "Epoch 209: train loss = 1916.150367, test loss = 3627.285216\n",
            "Epoch 210: train loss = 2293.835087, test loss = 5063.830826\n",
            "Epoch 211: train loss = 1942.181939, test loss = 1766.125173\n",
            "Epoch 212: train loss = 1655.350857, test loss = 3195.335921\n",
            "Epoch 213: train loss = 1638.852637, test loss = 1779.367972\n",
            "Epoch 214: train loss = 1544.104556, test loss = 1769.664196\n",
            "Epoch 215: train loss = 1571.884815, test loss = 1785.135089\n",
            "Epoch 216: train loss = 1609.625921, test loss = 1709.301527\n",
            "Epoch 217: train loss = 1567.085834, test loss = 1757.128608\n",
            "Epoch 218: train loss = 1517.373156, test loss = 1774.306990\n",
            "Epoch 219: train loss = 1565.869238, test loss = 1950.025842\n",
            "Epoch 220: train loss = 1631.622063, test loss = 1986.775896\n",
            "Epoch 221: train loss = 1579.884859, test loss = 1880.168998\n",
            "Epoch 222: train loss = 1612.506253, test loss = 1797.552258\n",
            "Epoch 223: train loss = 1540.865644, test loss = 1782.287623\n",
            "Epoch 224: train loss = 1588.958603, test loss = 1764.783620\n",
            "Epoch 225: train loss = 1530.948558, test loss = 1705.362381\n",
            "Epoch 226: train loss = 1545.183772, test loss = 1744.676837\n",
            "Epoch 227: train loss = 1740.636140, test loss = 3279.862988\n",
            "Epoch 228: train loss = 1722.483420, test loss = 1858.514610\n",
            "Epoch 229: train loss = 1655.686632, test loss = 2104.152770\n",
            "Epoch 230: train loss = 1727.300863, test loss = 2350.141160\n",
            "Epoch 231: train loss = 1603.070255, test loss = 1817.682901\n",
            "Epoch 232: train loss = 1598.822169, test loss = 1986.449162\n",
            "Epoch 233: train loss = 1534.100137, test loss = 1740.948786\n",
            "Epoch 234: train loss = 1612.771578, test loss = 1781.412977\n",
            "Epoch 235: train loss = 1591.644437, test loss = 1963.457041\n",
            "Epoch 236: train loss = 1587.102855, test loss = 1959.864900\n",
            "Epoch 237: train loss = 1760.451210, test loss = 2691.680547\n",
            "Epoch 238: train loss = 1751.215508, test loss = 2319.450675\n",
            "Epoch 239: train loss = 1656.164980, test loss = 1847.695939\n",
            "Epoch 240: train loss = 1593.144286, test loss = 1742.349530\n",
            "Epoch 241: train loss = 1577.094434, test loss = 1907.172167\n",
            "Epoch 242: train loss = 1567.279084, test loss = 1794.151717\n",
            "Epoch 243: train loss = 1559.878720, test loss = 2080.966267\n",
            "Epoch 244: train loss = 1676.199643, test loss = 2272.886137\n",
            "Epoch 245: train loss = 1609.989701, test loss = 1851.282081\n",
            "Epoch 246: train loss = 1651.081843, test loss = 2353.628051\n",
            "Epoch 247: train loss = 1717.620429, test loss = 1885.407033\n",
            "Epoch 248: train loss = 1539.802386, test loss = 1781.043646\n",
            "Epoch 249: train loss = 1632.904788, test loss = 2064.161168\n",
            "Epoch 250: train loss = 1630.796352, test loss = 2178.233357\n",
            "Epoch 251: train loss = 1580.407561, test loss = 1729.762328\n",
            "Epoch 252: train loss = 1585.726246, test loss = 1983.326757\n",
            "Epoch 253: train loss = 1563.297918, test loss = 1842.415383\n",
            "Epoch 254: train loss = 1604.077321, test loss = 1754.834872\n",
            "Epoch 255: train loss = 1742.978663, test loss = 2378.496883\n",
            "Epoch 256: train loss = 1597.471327, test loss = 1814.978170\n",
            "Epoch 257: train loss = 1612.142174, test loss = 2103.443311\n",
            "Epoch 258: train loss = 1639.277022, test loss = 1888.654431\n",
            "Epoch 259: train loss = 1599.402569, test loss = 1721.138744\n",
            "Epoch 260: train loss = 1673.658375, test loss = 2261.105227\n",
            "Epoch 261: train loss = 1926.822989, test loss = 2538.575357\n",
            "Epoch 262: train loss = 1676.915663, test loss = 1847.467180\n",
            "Epoch 263: train loss = 1574.156671, test loss = 1733.528619\n",
            "Epoch 264: train loss = 1637.053360, test loss = 1820.850621\n",
            "Epoch 265: train loss = 1589.512394, test loss = 1762.616206\n",
            "Epoch 266: train loss = 1560.927394, test loss = 1837.855081\n",
            "Epoch 267: train loss = 1531.315801, test loss = 1729.826284\n",
            "Epoch 268: train loss = 1572.012470, test loss = 2063.856252\n",
            "Epoch 269: train loss = 1597.677113, test loss = 2007.483494\n",
            "Epoch 270: train loss = 1568.225851, test loss = 1862.214709\n",
            "Epoch 271: train loss = 1614.213882, test loss = 1930.998425\n",
            "Epoch 272: train loss = 1540.733253, test loss = 1908.903004\n",
            "Epoch 273: train loss = 1569.020742, test loss = 1722.701715\n",
            "Epoch 274: train loss = 1581.438690, test loss = 1712.057189\n",
            "Epoch 275: train loss = 1550.788669, test loss = 1735.611788\n",
            "Epoch 276: train loss = 1581.382935, test loss = 1724.391506\n",
            "Epoch 277: train loss = 1729.312710, test loss = 2410.530742\n",
            "Epoch 278: train loss = 1914.013292, test loss = 4398.199407\n",
            "Epoch 279: train loss = 1824.821386, test loss = 1759.092101\n",
            "Epoch 280: train loss = 1598.069832, test loss = 2167.449840\n",
            "Epoch 281: train loss = 1561.197354, test loss = 1750.788025\n",
            "Epoch 282: train loss = 1543.468845, test loss = 1773.138556\n",
            "Epoch 283: train loss = 1540.484261, test loss = 1777.227727\n",
            "Epoch 284: train loss = 1584.468336, test loss = 1772.001926\n",
            "Epoch 285: train loss = 1548.552592, test loss = 1770.068040\n",
            "Epoch 286: train loss = 1586.947662, test loss = 1888.386377\n",
            "Epoch 287: train loss = 1748.587130, test loss = 2731.747507\n",
            "Epoch 288: train loss = 1628.418577, test loss = 1904.731135\n",
            "Epoch 289: train loss = 1588.536584, test loss = 1766.395697\n",
            "Epoch 290: train loss = 1755.460045, test loss = 2997.243456\n",
            "Epoch 291: train loss = 1649.696909, test loss = 1918.762153\n",
            "Epoch 292: train loss = 1596.883697, test loss = 2044.381959\n",
            "Epoch 293: train loss = 1676.946310, test loss = 2057.801535\n",
            "Epoch 294: train loss = 1585.567474, test loss = 1743.610642\n",
            "Epoch 295: train loss = 1527.323528, test loss = 1765.005699\n",
            "Epoch 296: train loss = 1603.445378, test loss = 1859.681742\n",
            "Epoch 297: train loss = 1775.989767, test loss = 3440.057091\n",
            "Epoch 298: train loss = 1758.395639, test loss = 1745.394784\n",
            "Epoch 299: train loss = 1558.614691, test loss = 1846.713263\n",
            "Epoch 300: train loss = 1580.491308, test loss = 1730.132589\n",
            "Epoch 301: train loss = 1553.514393, test loss = 1910.242009\n",
            "Epoch 302: train loss = 1691.242290, test loss = 2040.039883\n",
            "Epoch 303: train loss = 1575.347118, test loss = 2138.404593\n",
            "Epoch 304: train loss = 1711.302961, test loss = 3125.823566\n",
            "Epoch 305: train loss = 1905.640796, test loss = 2885.280382\n",
            "Epoch 306: train loss = 1685.882461, test loss = 1766.851734\n",
            "Epoch 307: train loss = 1617.395712, test loss = 2609.105816\n",
            "Epoch 308: train loss = 1671.159373, test loss = 1756.979628\n",
            "Epoch 309: train loss = 1579.986898, test loss = 1964.112855\n",
            "Epoch 310: train loss = 1565.553845, test loss = 1719.417872\n",
            "Epoch 311: train loss = 1666.194258, test loss = 1911.849526\n",
            "Epoch 312: train loss = 1573.813074, test loss = 1773.690272\n",
            "Epoch 313: train loss = 1522.319473, test loss = 1881.752362\n",
            "Epoch 314: train loss = 1601.092427, test loss = 1775.980252\n",
            "Epoch 315: train loss = 1548.443098, test loss = 1713.410637\n",
            "Epoch 316: train loss = 1536.383101, test loss = 1715.137182\n",
            "Epoch 317: train loss = 1540.448982, test loss = 1722.838574\n",
            "Epoch 318: train loss = 1605.832620, test loss = 2259.359130\n",
            "Epoch 319: train loss = 1725.043942, test loss = 2660.267151\n",
            "Epoch 320: train loss = 1679.710432, test loss = 2027.012833\n",
            "Epoch 321: train loss = 1706.944125, test loss = 2281.669743\n",
            "Epoch 322: train loss = 1699.439818, test loss = 2196.305905\n",
            "Epoch 323: train loss = 1581.654533, test loss = 1752.583091\n",
            "Epoch 324: train loss = 1562.725212, test loss = 1923.115886\n",
            "Epoch 325: train loss = 1613.616571, test loss = 1852.123375\n",
            "Epoch 326: train loss = 1660.943700, test loss = 2118.235153\n",
            "Epoch 327: train loss = 1647.360632, test loss = 2321.555927\n",
            "Epoch 328: train loss = 1572.772821, test loss = 1744.193112\n",
            "Epoch 329: train loss = 1683.966086, test loss = 3764.677547\n",
            "Epoch 330: train loss = 1897.400851, test loss = 2686.460711\n",
            "Epoch 331: train loss = 1638.354155, test loss = 1910.948093\n",
            "Epoch 332: train loss = 1675.853690, test loss = 2008.035049\n",
            "Epoch 333: train loss = 1594.800502, test loss = 1797.367341\n",
            "Epoch 334: train loss = 1751.947122, test loss = 2723.549683\n",
            "Epoch 335: train loss = 1652.890672, test loss = 1818.718134\n",
            "Epoch 336: train loss = 1538.983934, test loss = 1740.592478\n",
            "Epoch 337: train loss = 1573.672119, test loss = 2637.836573\n",
            "Epoch 338: train loss = 1674.904448, test loss = 3156.320553\n",
            "Epoch 339: train loss = 1643.180307, test loss = 1747.764602\n",
            "Epoch 340: train loss = 1622.730626, test loss = 2072.143967\n",
            "Epoch 341: train loss = 1597.655448, test loss = 1855.022888\n",
            "Epoch 342: train loss = 1621.138071, test loss = 1926.799573\n",
            "Epoch 343: train loss = 1591.205493, test loss = 1734.553735\n",
            "Epoch 344: train loss = 1612.805511, test loss = 1792.187854\n",
            "Epoch 345: train loss = 1682.840672, test loss = 2334.957562\n",
            "Epoch 346: train loss = 1608.830900, test loss = 2229.910853\n",
            "Epoch 347: train loss = 1773.037974, test loss = 2487.602245\n",
            "Epoch 348: train loss = 1608.386881, test loss = 1947.012394\n",
            "Epoch 349: train loss = 1624.059044, test loss = 1846.015331\n",
            "Epoch 350: train loss = 1650.582044, test loss = 2172.307983\n",
            "Epoch 351: train loss = 1594.064224, test loss = 1749.468284\n",
            "Epoch 352: train loss = 1593.166294, test loss = 2224.226589\n",
            "Epoch 353: train loss = 1577.085747, test loss = 1816.564841\n",
            "Epoch 354: train loss = 1672.788491, test loss = 2846.533239\n",
            "Epoch 355: train loss = 1657.855353, test loss = 1856.443192\n",
            "Epoch 356: train loss = 1563.089886, test loss = 1745.864250\n",
            "Epoch 357: train loss = 1645.666273, test loss = 2307.353543\n",
            "Epoch 358: train loss = 1647.036725, test loss = 2014.477045\n",
            "Epoch 359: train loss = 1599.595327, test loss = 1803.063273\n",
            "Epoch 360: train loss = 1549.769458, test loss = 1860.056911\n",
            "Epoch 361: train loss = 1666.743969, test loss = 2469.096813\n",
            "Epoch 362: train loss = 1581.443393, test loss = 1853.955621\n",
            "Epoch 363: train loss = 1637.336831, test loss = 2166.583982\n",
            "Epoch 364: train loss = 1747.967392, test loss = 2413.952317\n",
            "Epoch 365: train loss = 1608.513588, test loss = 1907.005527\n",
            "Epoch 366: train loss = 1587.338167, test loss = 1954.232546\n",
            "Epoch 367: train loss = 1636.174111, test loss = 1911.017904\n",
            "Epoch 368: train loss = 1708.573702, test loss = 4214.658340\n",
            "Epoch 369: train loss = 1735.799334, test loss = 2263.287677\n",
            "Epoch 370: train loss = 1571.978715, test loss = 1831.733795\n",
            "Epoch 371: train loss = 1613.547839, test loss = 2114.300087\n",
            "Epoch 372: train loss = 1624.648392, test loss = 1922.539281\n",
            "Epoch 373: train loss = 1563.615191, test loss = 1857.797796\n",
            "Epoch 374: train loss = 1720.318297, test loss = 3117.680757\n",
            "Epoch 375: train loss = 1658.618456, test loss = 1733.028207\n",
            "Epoch 376: train loss = 1673.209432, test loss = 3127.475832\n",
            "Epoch 377: train loss = 1739.552200, test loss = 1852.931146\n",
            "Epoch 378: train loss = 1740.033483, test loss = 2991.532582\n",
            "Epoch 379: train loss = 1643.082376, test loss = 1717.199814\n",
            "Epoch 380: train loss = 1584.073708, test loss = 2009.504742\n",
            "Epoch 381: train loss = 1684.093423, test loss = 2381.032735\n",
            "Epoch 382: train loss = 1619.361201, test loss = 1989.566285\n",
            "Epoch 383: train loss = 1666.842209, test loss = 2765.434762\n",
            "Epoch 384: train loss = 1675.693618, test loss = 1923.262585\n",
            "Epoch 385: train loss = 1596.838357, test loss = 1747.108596\n",
            "Epoch 386: train loss = 1538.341852, test loss = 1794.948342\n",
            "Epoch 387: train loss = 1724.157322, test loss = 2524.237006\n",
            "Epoch 388: train loss = 1630.459572, test loss = 1866.815678\n",
            "Epoch 389: train loss = 1762.081754, test loss = 2092.924122\n",
            "Epoch 390: train loss = 1787.827214, test loss = 2942.774431\n",
            "Epoch 391: train loss = 1610.452853, test loss = 1758.707221\n",
            "Epoch 392: train loss = 1597.901322, test loss = 1957.972593\n",
            "Epoch 393: train loss = 1567.405650, test loss = 2141.713171\n",
            "Epoch 394: train loss = 1592.176095, test loss = 1755.431493\n",
            "Epoch 395: train loss = 1660.887436, test loss = 2474.107423\n",
            "Epoch 396: train loss = 1581.614190, test loss = 1706.149175\n",
            "Epoch 397: train loss = 1567.367467, test loss = 1794.280514\n",
            "Epoch 398: train loss = 1619.598960, test loss = 2290.366456\n",
            "Epoch 399: train loss = 1585.273429, test loss = 1812.557122\n",
            "Epoch 400: train loss = 1570.200813, test loss = 1946.887784\n",
            "Epoch 401: train loss = 1576.003447, test loss = 1705.844385\n",
            "Epoch 402: train loss = 2059.081135, test loss = 4357.565531\n",
            "Epoch 403: train loss = 1835.548459, test loss = 2103.076284\n",
            "Epoch 404: train loss = 1609.271255, test loss = 1845.258943\n",
            "Epoch 405: train loss = 1602.181323, test loss = 1914.567321\n",
            "Epoch 406: train loss = 1549.112617, test loss = 1803.282651\n",
            "Epoch 407: train loss = 1572.826698, test loss = 1705.906253\n",
            "Epoch 408: train loss = 1586.608490, test loss = 1815.902232\n",
            "Epoch 409: train loss = 1543.424888, test loss = 1728.873899\n",
            "Epoch 410: train loss = 1521.423969, test loss = 1709.831943\n",
            "Epoch 411: train loss = 1655.718283, test loss = 3127.082032\n",
            "Epoch 412: train loss = 1773.023025, test loss = 1816.974611\n",
            "Epoch 413: train loss = 1541.385131, test loss = 1765.248097\n",
            "Epoch 414: train loss = 1560.831671, test loss = 1709.257715\n",
            "Epoch 415: train loss = 1540.019266, test loss = 1721.003912\n",
            "Epoch 416: train loss = 1586.164566, test loss = 1995.336886\n",
            "Epoch 417: train loss = 1610.629665, test loss = 1953.710522\n",
            "Epoch 418: train loss = 1553.826238, test loss = 1736.061746\n",
            "Epoch 419: train loss = 1523.871980, test loss = 1736.173639\n",
            "Epoch 420: train loss = 1577.451908, test loss = 1867.622579\n",
            "Epoch 421: train loss = 1606.420084, test loss = 1870.597338\n",
            "Epoch 422: train loss = 1577.680042, test loss = 1722.575520\n",
            "Epoch 423: train loss = 1597.039236, test loss = 2353.230656\n",
            "Epoch 424: train loss = 1602.192097, test loss = 1754.818479\n",
            "Epoch 425: train loss = 1540.765465, test loss = 1866.372156\n",
            "Epoch 426: train loss = 1533.282604, test loss = 1733.342951\n",
            "Epoch 427: train loss = 1597.692122, test loss = 1881.635730\n",
            "Epoch 428: train loss = 2009.752130, test loss = 4099.624427\n",
            "Epoch 429: train loss = 1848.089667, test loss = 2392.923229\n",
            "Epoch 430: train loss = 1831.729592, test loss = 1985.586591\n",
            "Epoch 431: train loss = 1615.061468, test loss = 1895.375792\n",
            "Epoch 432: train loss = 1536.069165, test loss = 1805.132255\n",
            "Epoch 433: train loss = 1522.119803, test loss = 1724.576308\n",
            "Epoch 434: train loss = 1585.804716, test loss = 1810.542397\n",
            "Epoch 435: train loss = 1536.830549, test loss = 1757.686533\n",
            "Epoch 436: train loss = 1545.104484, test loss = 1918.421516\n",
            "Epoch 437: train loss = 1641.498347, test loss = 2515.811055\n",
            "Epoch 438: train loss = 1629.452563, test loss = 1798.229137\n",
            "Epoch 439: train loss = 1566.277700, test loss = 1795.269151\n",
            "Epoch 440: train loss = 1566.760617, test loss = 1856.061792\n",
            "Epoch 441: train loss = 1622.060967, test loss = 1875.594868\n",
            "Epoch 442: train loss = 1644.425920, test loss = 2328.251741\n",
            "Epoch 443: train loss = 1569.923636, test loss = 1742.399273\n",
            "Epoch 444: train loss = 1679.582928, test loss = 2767.600253\n",
            "Epoch 445: train loss = 1735.529157, test loss = 2433.274412\n",
            "Epoch 446: train loss = 1680.881450, test loss = 2106.044424\n",
            "Epoch 447: train loss = 1676.214746, test loss = 3021.874343\n",
            "Epoch 448: train loss = 1600.455202, test loss = 1934.124218\n",
            "Epoch 449: train loss = 1573.982877, test loss = 1957.150896\n",
            "Epoch 450: train loss = 1588.415456, test loss = 2064.174584\n",
            "Epoch 451: train loss = 1552.991588, test loss = 1840.355004\n",
            "Epoch 452: train loss = 1527.593410, test loss = 1934.970207\n",
            "Epoch 453: train loss = 1583.031327, test loss = 1794.297981\n",
            "Epoch 454: train loss = 1565.520095, test loss = 1786.586312\n",
            "Epoch 455: train loss = 1597.716496, test loss = 1915.597343\n",
            "Epoch 456: train loss = 1552.927449, test loss = 1771.964470\n",
            "Epoch 457: train loss = 1537.599609, test loss = 1793.417898\n",
            "Epoch 458: train loss = 1657.798917, test loss = 2343.085900\n",
            "Epoch 459: train loss = 1583.539617, test loss = 2091.988660\n",
            "Epoch 460: train loss = 1558.540352, test loss = 1771.958109\n",
            "Epoch 461: train loss = 1735.631076, test loss = 2273.444227\n",
            "Epoch 462: train loss = 1610.983293, test loss = 1981.216508\n",
            "Epoch 463: train loss = 1663.963786, test loss = 2368.823819\n",
            "Epoch 464: train loss = 1623.442793, test loss = 1759.792700\n",
            "Epoch 465: train loss = 1659.452201, test loss = 2167.678185\n",
            "Epoch 466: train loss = 1656.904116, test loss = 2290.294253\n",
            "Epoch 467: train loss = 1565.952248, test loss = 2039.154919\n",
            "Epoch 468: train loss = 1602.719879, test loss = 1812.517826\n",
            "Epoch 469: train loss = 1596.069429, test loss = 1928.343238\n",
            "Epoch 470: train loss = 1585.199428, test loss = 1859.264606\n",
            "Epoch 471: train loss = 1547.702188, test loss = 1749.171742\n",
            "Epoch 472: train loss = 1650.244390, test loss = 3115.513774\n",
            "Epoch 473: train loss = 1745.032845, test loss = 1907.855661\n",
            "Epoch 474: train loss = 1610.354197, test loss = 2219.109045\n",
            "Epoch 475: train loss = 1699.641445, test loss = 2083.793083\n",
            "Epoch 476: train loss = 1594.501706, test loss = 1762.689718\n",
            "Epoch 477: train loss = 1633.022474, test loss = 2092.079404\n",
            "Epoch 478: train loss = 1572.409882, test loss = 1925.588090\n",
            "Epoch 479: train loss = 1570.563635, test loss = 1767.844608\n",
            "Epoch 480: train loss = 1543.533343, test loss = 1952.423568\n",
            "Epoch 481: train loss = 1551.403926, test loss = 1817.924124\n",
            "Epoch 482: train loss = 1536.688859, test loss = 1865.138933\n",
            "Epoch 483: train loss = 1656.004675, test loss = 2149.195340\n",
            "Epoch 484: train loss = 1726.616194, test loss = 2874.077595\n",
            "Epoch 485: train loss = 1695.884010, test loss = 2151.440665\n",
            "Epoch 486: train loss = 1834.840770, test loss = 3457.095432\n",
            "Epoch 487: train loss = 1675.112245, test loss = 1975.944541\n",
            "Epoch 488: train loss = 1560.426445, test loss = 1767.675919\n",
            "Epoch 489: train loss = 1537.974616, test loss = 1751.065839\n",
            "Epoch 490: train loss = 1548.653742, test loss = 2097.114121\n",
            "Epoch 491: train loss = 1608.075179, test loss = 1924.751318\n",
            "Epoch 492: train loss = 1549.987733, test loss = 1784.575323\n",
            "Epoch 493: train loss = 1719.586123, test loss = 2429.097640\n",
            "Epoch 494: train loss = 1715.128541, test loss = 2920.596084\n",
            "Epoch 495: train loss = 1557.401570, test loss = 1775.116610\n",
            "Epoch 496: train loss = 1593.417311, test loss = 2206.770210\n",
            "Epoch 497: train loss = 1560.280867, test loss = 1764.799978\n",
            "Epoch 498: train loss = 1559.847112, test loss = 2208.638677\n",
            "Epoch 499: train loss = 1595.626161, test loss = 2015.523610\n",
            "Epoch 500: train loss = 1578.050908, test loss = 1817.932400\n",
            "Epoch 501: train loss = 1615.345161, test loss = 1917.816756\n",
            "Epoch 502: train loss = 1554.563701, test loss = 1751.277399\n",
            "Epoch 503: train loss = 1550.797963, test loss = 1804.335801\n",
            "Epoch 504: train loss = 1560.285093, test loss = 1754.163141\n",
            "Epoch 505: train loss = 1605.460165, test loss = 2482.941594\n",
            "Epoch 506: train loss = 1617.721260, test loss = 2032.222594\n",
            "Epoch 507: train loss = 1584.339209, test loss = 2195.706221\n",
            "Epoch 508: train loss = 1590.861593, test loss = 1826.661028\n",
            "Epoch 509: train loss = 1571.552094, test loss = 1952.354262\n",
            "Epoch 510: train loss = 1635.656557, test loss = 2583.699487\n",
            "Epoch 511: train loss = 1620.680533, test loss = 2502.933378\n",
            "Epoch 512: train loss = 1585.216533, test loss = 1798.688315\n",
            "Epoch 513: train loss = 1559.635462, test loss = 1904.949324\n",
            "Epoch 514: train loss = 1628.909763, test loss = 2391.275726\n",
            "Epoch 515: train loss = 1797.574381, test loss = 2371.443067\n",
            "Epoch 516: train loss = 1688.033191, test loss = 2180.371350\n",
            "Epoch 517: train loss = 1629.933173, test loss = 1779.127114\n",
            "Epoch 518: train loss = 1549.025550, test loss = 1812.070614\n",
            "Epoch 519: train loss = 1544.433167, test loss = 1832.891394\n",
            "Epoch 520: train loss = 1539.554301, test loss = 2018.761843\n",
            "Epoch 521: train loss = 1578.381671, test loss = 1867.719130\n",
            "Epoch 522: train loss = 1643.870066, test loss = 2305.351422\n",
            "Epoch 523: train loss = 1668.663916, test loss = 1935.478854\n",
            "Epoch 524: train loss = 1549.124942, test loss = 1823.008517\n",
            "Epoch 525: train loss = 1570.571428, test loss = 1894.561397\n",
            "Epoch 526: train loss = 1655.428725, test loss = 2270.478245\n",
            "Epoch 527: train loss = 1584.009746, test loss = 1759.247187\n",
            "Epoch 528: train loss = 1533.640524, test loss = 1845.404111\n",
            "Epoch 529: train loss = 1519.239569, test loss = 1764.731103\n",
            "Epoch 530: train loss = 1557.471496, test loss = 1901.450255\n",
            "Epoch 531: train loss = 1776.116345, test loss = 4144.107021\n",
            "Epoch 532: train loss = 1815.147989, test loss = 2350.417775\n",
            "Epoch 533: train loss = 1590.649497, test loss = 1877.552269\n",
            "Epoch 534: train loss = 1719.161427, test loss = 3337.528947\n",
            "Epoch 535: train loss = 1816.660668, test loss = 2466.956157\n",
            "Epoch 536: train loss = 1577.993117, test loss = 1830.655785\n",
            "Epoch 537: train loss = 1557.191839, test loss = 2033.069267\n",
            "Epoch 538: train loss = 1643.813121, test loss = 2573.169926\n",
            "Epoch 539: train loss = 1627.002703, test loss = 2304.743674\n",
            "Epoch 540: train loss = 1543.320866, test loss = 2139.005826\n",
            "Epoch 541: train loss = 1565.359091, test loss = 1823.803444\n",
            "Epoch 542: train loss = 1600.058655, test loss = 1843.783716\n",
            "Epoch 543: train loss = 1595.322597, test loss = 2009.487833\n",
            "Epoch 544: train loss = 1688.524588, test loss = 2625.307801\n",
            "Epoch 545: train loss = 1621.542073, test loss = 1931.745591\n",
            "Epoch 546: train loss = 1611.563255, test loss = 2453.702483\n",
            "Epoch 547: train loss = 1624.306399, test loss = 1808.738830\n",
            "Epoch 548: train loss = 1554.222186, test loss = 1783.452787\n",
            "Epoch 549: train loss = 1758.150569, test loss = 2653.145875\n",
            "Epoch 550: train loss = 1664.935938, test loss = 2489.003683\n",
            "Epoch 551: train loss = 1587.995368, test loss = 1781.042709\n",
            "Epoch 552: train loss = 1753.096096, test loss = 3238.428009\n",
            "Epoch 553: train loss = 1720.607770, test loss = 2310.263296\n",
            "Epoch 554: train loss = 1633.504456, test loss = 1935.302996\n",
            "Epoch 555: train loss = 1822.034251, test loss = 3361.809899\n",
            "Epoch 556: train loss = 1730.697260, test loss = 2018.259579\n",
            "Epoch 557: train loss = 1609.617735, test loss = 1919.250098\n",
            "Epoch 558: train loss = 1550.478311, test loss = 1943.010395\n",
            "Epoch 559: train loss = 1541.500600, test loss = 1885.368538\n",
            "Epoch 560: train loss = 1707.721352, test loss = 2370.155375\n",
            "Epoch 561: train loss = 1662.835538, test loss = 1926.295022\n",
            "Epoch 562: train loss = 1659.434176, test loss = 2245.174974\n",
            "Epoch 563: train loss = 1633.540851, test loss = 2054.728369\n",
            "Epoch 564: train loss = 1677.931646, test loss = 2174.987830\n",
            "Epoch 565: train loss = 1590.728235, test loss = 1819.474155\n",
            "Epoch 566: train loss = 1546.266509, test loss = 1865.058526\n",
            "Epoch 567: train loss = 1552.377515, test loss = 2058.323738\n",
            "Epoch 568: train loss = 1719.550003, test loss = 2515.490907\n",
            "Epoch 569: train loss = 1661.842808, test loss = 2189.578490\n",
            "Epoch 570: train loss = 1546.213153, test loss = 1890.344772\n",
            "Epoch 571: train loss = 1648.450752, test loss = 3005.476978\n",
            "Epoch 572: train loss = 1720.347139, test loss = 1934.814812\n",
            "Epoch 573: train loss = 1575.374611, test loss = 2010.205498\n",
            "Epoch 574: train loss = 1619.841401, test loss = 2399.815772\n",
            "Epoch 575: train loss = 1597.451231, test loss = 2069.255500\n",
            "Epoch 576: train loss = 1560.805139, test loss = 2164.268480\n",
            "Epoch 577: train loss = 1577.874936, test loss = 1888.738642\n",
            "Epoch 578: train loss = 1665.334127, test loss = 2925.936023\n",
            "Epoch 579: train loss = 1847.384074, test loss = 2783.036580\n",
            "Epoch 580: train loss = 1776.024578, test loss = 3701.369930\n",
            "Epoch 581: train loss = 1750.934190, test loss = 1856.970607\n",
            "Epoch 582: train loss = 1547.577398, test loss = 1766.062835\n",
            "Epoch 583: train loss = 1547.071255, test loss = 1892.353513\n",
            "Epoch 584: train loss = 1586.829964, test loss = 2233.332070\n",
            "Epoch 585: train loss = 1613.818405, test loss = 1947.377655\n",
            "Epoch 586: train loss = 1650.151985, test loss = 2332.465051\n",
            "Epoch 587: train loss = 1635.796127, test loss = 3129.567434\n",
            "Epoch 588: train loss = 1687.387651, test loss = 1791.953548\n",
            "Epoch 589: train loss = 1746.038433, test loss = 2964.729076\n",
            "Epoch 590: train loss = 1634.564253, test loss = 2234.743688\n",
            "Epoch 591: train loss = 1577.552765, test loss = 1992.203869\n",
            "Epoch 592: train loss = 1559.086376, test loss = 1804.969131\n",
            "Epoch 593: train loss = 1537.101512, test loss = 2006.349647\n",
            "Epoch 594: train loss = 1554.962424, test loss = 1772.176219\n",
            "Epoch 595: train loss = 1658.748508, test loss = 2162.377796\n",
            "Epoch 596: train loss = 1642.484302, test loss = 2266.917595\n",
            "Epoch 597: train loss = 1764.564864, test loss = 2702.706617\n",
            "Epoch 598: train loss = 1661.226169, test loss = 2010.183018\n",
            "Epoch 599: train loss = 1624.058804, test loss = 2403.443698\n",
            "Epoch 600: train loss = 1585.583042, test loss = 1832.255474\n",
            "Epoch 601: train loss = 1554.268167, test loss = 1890.934301\n",
            "Epoch 602: train loss = 1566.977983, test loss = 1804.354944\n",
            "Epoch 603: train loss = 1713.104308, test loss = 2912.904905\n",
            "Epoch 604: train loss = 1655.715142, test loss = 1786.967900\n",
            "Epoch 605: train loss = 1560.915769, test loss = 2101.222436\n",
            "Epoch 606: train loss = 1675.329305, test loss = 2737.596301\n",
            "Epoch 607: train loss = 1657.175527, test loss = 1929.459599\n",
            "Epoch 608: train loss = 1634.445390, test loss = 2901.572722\n",
            "Epoch 609: train loss = 1670.770806, test loss = 1779.508958\n",
            "Epoch 610: train loss = 1549.706375, test loss = 1789.681550\n",
            "Epoch 611: train loss = 1922.750034, test loss = 3774.261812\n",
            "Epoch 612: train loss = 1853.316419, test loss = 1770.712539\n",
            "Epoch 613: train loss = 1539.956557, test loss = 2118.454051\n",
            "Epoch 614: train loss = 1577.311859, test loss = 1898.377312\n",
            "Epoch 615: train loss = 1692.378791, test loss = 3705.832790\n",
            "Epoch 616: train loss = 1696.703179, test loss = 1786.163694\n",
            "Epoch 617: train loss = 1541.670497, test loss = 1779.944010\n",
            "Epoch 618: train loss = 1520.472908, test loss = 1802.842853\n",
            "Epoch 619: train loss = 1531.032377, test loss = 1788.206700\n",
            "Epoch 620: train loss = 1574.427143, test loss = 1893.755081\n",
            "Epoch 621: train loss = 1531.843845, test loss = 1801.465280\n",
            "Epoch 622: train loss = 1522.984748, test loss = 1968.833973\n",
            "Epoch 623: train loss = 1559.797563, test loss = 2008.033321\n",
            "Epoch 624: train loss = 1576.184258, test loss = 2011.753195\n",
            "Epoch 625: train loss = 1734.671597, test loss = 2587.541334\n",
            "Epoch 626: train loss = 1605.180523, test loss = 1819.380022\n",
            "Epoch 627: train loss = 1702.380858, test loss = 2495.612818\n",
            "Epoch 628: train loss = 1665.572778, test loss = 2055.469365\n",
            "Epoch 629: train loss = 1520.674049, test loss = 1773.672552\n",
            "Epoch 630: train loss = 1538.927745, test loss = 2026.389424\n",
            "Epoch 631: train loss = 1558.740520, test loss = 1889.231600\n",
            "Epoch 632: train loss = 1526.917302, test loss = 1787.543099\n",
            "Epoch 633: train loss = 1692.167657, test loss = 3343.089601\n",
            "Epoch 634: train loss = 1810.286780, test loss = 3078.458457\n",
            "Epoch 635: train loss = 1597.011474, test loss = 1876.115471\n",
            "Epoch 636: train loss = 1588.331646, test loss = 1960.188429\n",
            "Epoch 637: train loss = 1679.062251, test loss = 2562.911830\n",
            "Epoch 638: train loss = 1646.240546, test loss = 1868.240251\n",
            "Epoch 639: train loss = 1552.909586, test loss = 1863.675501\n",
            "Epoch 640: train loss = 1543.067499, test loss = 1811.986419\n",
            "Epoch 641: train loss = 1678.640887, test loss = 3744.031591\n",
            "Epoch 642: train loss = 1722.471180, test loss = 1920.097968\n",
            "Epoch 643: train loss = 1630.098548, test loss = 1845.224816\n",
            "Epoch 644: train loss = 1589.035637, test loss = 1872.923738\n",
            "Epoch 645: train loss = 1513.265282, test loss = 1839.836170\n",
            "Epoch 646: train loss = 1596.127720, test loss = 2159.354045\n",
            "Epoch 647: train loss = 1561.688130, test loss = 1764.656423\n",
            "Epoch 648: train loss = 1634.321406, test loss = 2234.731862\n",
            "Epoch 649: train loss = 1547.748679, test loss = 1909.477142\n",
            "Epoch 650: train loss = 1606.909577, test loss = 1961.206759\n",
            "Epoch 651: train loss = 1657.724883, test loss = 2174.895951\n",
            "Epoch 652: train loss = 1701.815079, test loss = 2394.188251\n",
            "Epoch 653: train loss = 1609.035437, test loss = 1782.339863\n",
            "Epoch 654: train loss = 1649.714446, test loss = 2170.496420\n",
            "Epoch 655: train loss = 1629.187399, test loss = 1773.635736\n",
            "Epoch 656: train loss = 1583.277263, test loss = 1927.403209\n",
            "Epoch 657: train loss = 1713.095718, test loss = 3118.743289\n",
            "Epoch 658: train loss = 1806.461621, test loss = 1816.211097\n",
            "Epoch 659: train loss = 1559.509318, test loss = 1985.026318\n",
            "Epoch 660: train loss = 1580.569395, test loss = 1887.187220\n",
            "Epoch 661: train loss = 1694.145576, test loss = 2925.356475\n",
            "Epoch 662: train loss = 1666.797814, test loss = 2290.154625\n",
            "Epoch 663: train loss = 1563.535190, test loss = 1841.930534\n",
            "Epoch 664: train loss = 1910.949868, test loss = 4218.948777\n",
            "Epoch 665: train loss = 2036.801355, test loss = 3362.287035\n",
            "Epoch 666: train loss = 2002.401983, test loss = 2463.274395\n",
            "Epoch 667: train loss = 1667.051893, test loss = 2094.198780\n",
            "Epoch 668: train loss = 1727.172499, test loss = 3234.228349\n",
            "Epoch 669: train loss = 1737.529875, test loss = 2517.528674\n",
            "Epoch 670: train loss = 1597.480001, test loss = 1839.189917\n",
            "Epoch 671: train loss = 1815.441395, test loss = 3569.441011\n",
            "Epoch 672: train loss = 1857.543415, test loss = 2695.699914\n",
            "Epoch 673: train loss = 1717.708578, test loss = 1923.037206\n",
            "Epoch 674: train loss = 1567.983388, test loss = 1822.134182\n",
            "Epoch 675: train loss = 1571.362526, test loss = 1890.508026\n",
            "Epoch 676: train loss = 1558.525075, test loss = 2095.940874\n",
            "Epoch 677: train loss = 1566.323111, test loss = 1783.273744\n",
            "Epoch 678: train loss = 1561.876542, test loss = 1836.904211\n",
            "Epoch 679: train loss = 1572.541561, test loss = 1817.104498\n",
            "Epoch 680: train loss = 1848.612169, test loss = 2352.699005\n",
            "Epoch 681: train loss = 1644.108107, test loss = 1858.822679\n",
            "Epoch 682: train loss = 1580.385973, test loss = 1844.531271\n",
            "Epoch 683: train loss = 1651.547795, test loss = 2308.029507\n",
            "Epoch 684: train loss = 1658.416670, test loss = 2172.777479\n",
            "Epoch 685: train loss = 1680.489018, test loss = 2146.603464\n",
            "Epoch 686: train loss = 1768.068312, test loss = 2670.724534\n",
            "Epoch 687: train loss = 1752.122190, test loss = 3394.935815\n",
            "Epoch 688: train loss = 1681.107222, test loss = 1799.949318\n",
            "Epoch 689: train loss = 1590.065919, test loss = 1790.212276\n",
            "Epoch 690: train loss = 1631.802412, test loss = 2699.582781\n",
            "Epoch 691: train loss = 1689.931812, test loss = 1787.249234\n",
            "Epoch 692: train loss = 1549.352540, test loss = 2035.446226\n",
            "Epoch 693: train loss = 1555.483636, test loss = 1846.892288\n",
            "Epoch 694: train loss = 1722.643160, test loss = 1939.010342\n",
            "Epoch 695: train loss = 1614.514880, test loss = 1826.582917\n",
            "Epoch 696: train loss = 1512.289136, test loss = 1769.594735\n",
            "Epoch 697: train loss = 1764.269194, test loss = 2787.876013\n",
            "Epoch 698: train loss = 1638.321438, test loss = 1940.250062\n",
            "Epoch 699: train loss = 1560.648571, test loss = 2109.995371\n",
            "Epoch 700: train loss = 1669.339169, test loss = 2236.154557\n",
            "Epoch 701: train loss = 1565.118813, test loss = 1830.924805\n",
            "Epoch 702: train loss = 1567.392936, test loss = 2032.414129\n",
            "Epoch 703: train loss = 1516.313144, test loss = 1853.583126\n",
            "Epoch 704: train loss = 1543.167245, test loss = 1930.498282\n",
            "Epoch 705: train loss = 1707.706047, test loss = 2582.697065\n",
            "Epoch 706: train loss = 1650.909962, test loss = 2898.724063\n",
            "Epoch 707: train loss = 1884.842376, test loss = 4020.824692\n",
            "Epoch 708: train loss = 1732.177015, test loss = 1829.193147\n",
            "Epoch 709: train loss = 1646.757288, test loss = 3435.104074\n",
            "Epoch 710: train loss = 1689.131368, test loss = 2127.556932\n",
            "Epoch 711: train loss = 1755.195045, test loss = 2524.010222\n",
            "Epoch 712: train loss = 1582.706980, test loss = 1837.308427\n",
            "Epoch 713: train loss = 1562.152275, test loss = 1861.340962\n",
            "Epoch 714: train loss = 1728.513284, test loss = 2518.544924\n",
            "Epoch 715: train loss = 1801.654624, test loss = 2436.816144\n",
            "Epoch 716: train loss = 1590.404119, test loss = 2079.812745\n",
            "Epoch 717: train loss = 1596.957552, test loss = 1778.697504\n",
            "Epoch 718: train loss = 1587.767093, test loss = 1790.061864\n",
            "Epoch 719: train loss = 1534.402070, test loss = 1778.988645\n",
            "Epoch 720: train loss = 1591.412551, test loss = 2321.847847\n",
            "Epoch 721: train loss = 1566.772799, test loss = 1761.483444\n",
            "Epoch 722: train loss = 1566.961087, test loss = 1834.025493\n",
            "Epoch 723: train loss = 1567.800026, test loss = 3042.456806\n",
            "Epoch 724: train loss = 1636.917234, test loss = 2213.072080\n",
            "Epoch 725: train loss = 1548.536735, test loss = 1780.526419\n",
            "Epoch 726: train loss = 1818.056411, test loss = 2730.684507\n",
            "Epoch 727: train loss = 1621.859470, test loss = 1798.835768\n",
            "Epoch 728: train loss = 1518.659303, test loss = 1812.274542\n",
            "Epoch 729: train loss = 1552.495998, test loss = 1995.659318\n",
            "Epoch 730: train loss = 1628.259667, test loss = 2950.924718\n",
            "Epoch 731: train loss = 1645.377220, test loss = 1831.287816\n",
            "Epoch 732: train loss = 1692.179857, test loss = 4037.663851\n",
            "Epoch 733: train loss = 1722.321131, test loss = 1929.510151\n",
            "Epoch 734: train loss = 1513.779163, test loss = 1795.840526\n",
            "Epoch 735: train loss = 1684.803446, test loss = 2462.387023\n",
            "Epoch 736: train loss = 1710.127889, test loss = 1831.335324\n",
            "Epoch 737: train loss = 1525.568747, test loss = 1776.627302\n",
            "Epoch 738: train loss = 1534.484491, test loss = 1826.435623\n",
            "Epoch 739: train loss = 1538.601779, test loss = 1788.086822\n",
            "Epoch 740: train loss = 1579.902930, test loss = 1985.390179\n",
            "Epoch 741: train loss = 1575.531096, test loss = 2132.808812\n",
            "Epoch 742: train loss = 1684.042561, test loss = 3007.190344\n",
            "Epoch 743: train loss = 2216.139049, test loss = 5733.753243\n",
            "Epoch 744: train loss = 2023.326628, test loss = 1993.390725\n",
            "Epoch 745: train loss = 1772.394937, test loss = 2583.781061\n",
            "Epoch 746: train loss = 1581.261420, test loss = 1832.012760\n",
            "Epoch 747: train loss = 1552.423611, test loss = 1961.638470\n",
            "Epoch 748: train loss = 1536.866167, test loss = 1931.644867\n",
            "Epoch 749: train loss = 1562.587072, test loss = 2091.930049\n",
            "Epoch 750: train loss = 1606.661771, test loss = 1813.526516\n",
            "Epoch 751: train loss = 1522.489633, test loss = 1892.895750\n",
            "Epoch 752: train loss = 1535.268189, test loss = 1971.609480\n",
            "Epoch 753: train loss = 1579.027581, test loss = 1872.046708\n",
            "Epoch 754: train loss = 1497.427456, test loss = 1791.157188\n",
            "Epoch 755: train loss = 1515.487539, test loss = 1807.658394\n",
            "Epoch 756: train loss = 1541.599863, test loss = 1824.904597\n",
            "Epoch 757: train loss = 1598.005876, test loss = 2784.612893\n",
            "Epoch 758: train loss = 1624.234010, test loss = 1885.314446\n",
            "Epoch 759: train loss = 1588.528707, test loss = 1920.957177\n",
            "Epoch 760: train loss = 1562.885740, test loss = 1863.333858\n",
            "Epoch 761: train loss = 1541.436736, test loss = 1834.628621\n",
            "Epoch 762: train loss = 1547.996559, test loss = 2449.947967\n",
            "Epoch 763: train loss = 1615.567504, test loss = 1802.075457\n",
            "Epoch 764: train loss = 1518.897152, test loss = 2089.061139\n",
            "Epoch 765: train loss = 1518.261596, test loss = 1873.014357\n",
            "Epoch 766: train loss = 1609.341683, test loss = 2078.404043\n",
            "Epoch 767: train loss = 1585.531781, test loss = 2028.791467\n",
            "Epoch 768: train loss = 1565.811124, test loss = 1848.882064\n",
            "Epoch 769: train loss = 1632.279620, test loss = 2017.217056\n",
            "Epoch 770: train loss = 1610.669757, test loss = 1944.432319\n",
            "Epoch 771: train loss = 1572.206007, test loss = 2590.332358\n",
            "Epoch 772: train loss = 1712.770812, test loss = 2842.160506\n",
            "Epoch 773: train loss = 1655.170476, test loss = 1974.815749\n",
            "Epoch 774: train loss = 1631.477922, test loss = 2689.350541\n",
            "Epoch 775: train loss = 1583.630497, test loss = 2253.922997\n",
            "Epoch 776: train loss = 1574.746721, test loss = 1812.000756\n",
            "Epoch 777: train loss = 1547.128547, test loss = 1816.958437\n",
            "Epoch 778: train loss = 1502.413258, test loss = 1870.509372\n",
            "Epoch 779: train loss = 1560.102383, test loss = 1951.717334\n",
            "Epoch 780: train loss = 1551.266517, test loss = 1888.616771\n",
            "Epoch 781: train loss = 1576.398049, test loss = 1831.679931\n",
            "Epoch 782: train loss = 1631.150805, test loss = 3477.591783\n",
            "Epoch 783: train loss = 1767.536783, test loss = 1866.989619\n",
            "Epoch 784: train loss = 1568.439210, test loss = 1824.608511\n",
            "Epoch 785: train loss = 1513.553731, test loss = 1816.120299\n",
            "Epoch 786: train loss = 1574.078716, test loss = 1840.829601\n",
            "Epoch 787: train loss = 1560.784122, test loss = 1832.378779\n",
            "Epoch 788: train loss = 1584.628659, test loss = 3161.251911\n",
            "Epoch 789: train loss = 1701.704091, test loss = 2197.966837\n",
            "Epoch 790: train loss = 1632.573761, test loss = 2030.448715\n",
            "Epoch 791: train loss = 1545.638883, test loss = 1893.206738\n",
            "Epoch 792: train loss = 1492.666898, test loss = 1797.230863\n",
            "Epoch 793: train loss = 1553.872570, test loss = 1816.018218\n",
            "Epoch 794: train loss = 1726.646325, test loss = 2446.246455\n",
            "Epoch 795: train loss = 1728.034711, test loss = 2119.479128\n",
            "Epoch 796: train loss = 1628.429117, test loss = 2819.512973\n",
            "Epoch 797: train loss = 1637.390935, test loss = 1944.171643\n",
            "Epoch 798: train loss = 1561.243925, test loss = 2261.756857\n",
            "Epoch 799: train loss = 1599.164134, test loss = 1990.203208\n",
            "Epoch 800: train loss = 1555.035155, test loss = 1817.695649\n",
            "Epoch 801: train loss = 1508.618303, test loss = 1795.469786\n",
            "Epoch 802: train loss = 1534.420337, test loss = 1958.749150\n",
            "Epoch 803: train loss = 1553.466628, test loss = 2078.332654\n",
            "Epoch 804: train loss = 1700.020967, test loss = 3077.926884\n",
            "Epoch 805: train loss = 1704.162069, test loss = 2113.106394\n",
            "Epoch 806: train loss = 1580.565732, test loss = 1882.902454\n",
            "Epoch 807: train loss = 1608.781883, test loss = 2005.887668\n",
            "Epoch 808: train loss = 1649.187219, test loss = 1929.107443\n",
            "Epoch 809: train loss = 1591.544249, test loss = 2397.640474\n",
            "Epoch 810: train loss = 1640.484927, test loss = 2642.925389\n",
            "Epoch 811: train loss = 1622.512014, test loss = 1920.838629\n",
            "Epoch 812: train loss = 1702.710309, test loss = 3735.773394\n",
            "Epoch 813: train loss = 1719.156967, test loss = 2189.264579\n",
            "Epoch 814: train loss = 1602.939300, test loss = 1882.755776\n",
            "Epoch 815: train loss = 1750.995029, test loss = 2630.111260\n",
            "Epoch 816: train loss = 1670.771373, test loss = 2058.198456\n",
            "Epoch 817: train loss = 1583.995256, test loss = 1798.262394\n",
            "Epoch 818: train loss = 1582.799822, test loss = 2008.733135\n",
            "Epoch 819: train loss = 1532.027931, test loss = 1857.089490\n",
            "Epoch 820: train loss = 1520.406505, test loss = 2579.336066\n",
            "Epoch 821: train loss = 1619.467368, test loss = 1818.301496\n",
            "Epoch 822: train loss = 1948.434316, test loss = 3979.131510\n",
            "Epoch 823: train loss = 1823.814418, test loss = 1799.924986\n",
            "Epoch 824: train loss = 1564.215671, test loss = 2405.898554\n",
            "Epoch 825: train loss = 1632.701085, test loss = 1972.964877\n",
            "Epoch 826: train loss = 1848.522523, test loss = 3992.167302\n",
            "Epoch 827: train loss = 1851.765191, test loss = 1970.937152\n",
            "Epoch 828: train loss = 1747.025128, test loss = 3670.791620\n",
            "Epoch 829: train loss = 1776.340103, test loss = 2113.040579\n",
            "Epoch 830: train loss = 1749.865450, test loss = 3097.791146\n",
            "Epoch 831: train loss = 1851.993158, test loss = 2064.462540\n",
            "Epoch 832: train loss = 1583.624633, test loss = 1940.031145\n",
            "Epoch 833: train loss = 1634.848987, test loss = 2231.706621\n",
            "Epoch 834: train loss = 1613.273122, test loss = 1987.014680\n",
            "Epoch 835: train loss = 1486.406120, test loss = 1807.525563\n",
            "Epoch 836: train loss = 1579.148988, test loss = 2372.279900\n",
            "Epoch 837: train loss = 1698.211157, test loss = 2570.532399\n",
            "Epoch 838: train loss = 1801.856680, test loss = 2678.162953\n",
            "Epoch 839: train loss = 1702.441344, test loss = 2299.193215\n",
            "Epoch 840: train loss = 1609.132464, test loss = 2496.368431\n",
            "Epoch 841: train loss = 1670.967421, test loss = 2289.317253\n",
            "Epoch 842: train loss = 1618.123401, test loss = 1835.084147\n",
            "Epoch 843: train loss = 1589.110144, test loss = 2143.852728\n",
            "Epoch 844: train loss = 1616.675770, test loss = 2069.239521\n",
            "Epoch 845: train loss = 1594.943957, test loss = 1877.621197\n",
            "Epoch 846: train loss = 1758.948419, test loss = 2022.754907\n",
            "Epoch 847: train loss = 1676.329740, test loss = 2473.975614\n",
            "Epoch 848: train loss = 1587.268834, test loss = 2117.292699\n",
            "Epoch 849: train loss = 1662.901318, test loss = 1918.850549\n",
            "Epoch 850: train loss = 1563.446645, test loss = 1799.838840\n",
            "Epoch 851: train loss = 1600.287041, test loss = 1918.656842\n",
            "Epoch 852: train loss = 1557.591391, test loss = 1811.126004\n",
            "Epoch 853: train loss = 1517.153234, test loss = 1877.183631\n",
            "Epoch 854: train loss = 1542.132518, test loss = 1918.536883\n",
            "Epoch 855: train loss = 1555.207585, test loss = 1810.781695\n",
            "Epoch 856: train loss = 1600.475190, test loss = 2484.391108\n",
            "Epoch 857: train loss = 1639.973960, test loss = 1809.888636\n",
            "Epoch 858: train loss = 1566.068755, test loss = 1835.701179\n",
            "Epoch 859: train loss = 1577.817175, test loss = 2287.263380\n",
            "Epoch 860: train loss = 1556.180021, test loss = 1808.303114\n",
            "Epoch 861: train loss = 1594.999869, test loss = 1933.855323\n",
            "Epoch 862: train loss = 1601.498899, test loss = 2004.746097\n",
            "Epoch 863: train loss = 1563.972757, test loss = 1806.573207\n",
            "Epoch 864: train loss = 1532.401045, test loss = 1925.407900\n",
            "Epoch 865: train loss = 1522.656205, test loss = 1823.829314\n",
            "Epoch 866: train loss = 1510.908515, test loss = 1925.601142\n",
            "Epoch 867: train loss = 1590.425564, test loss = 1947.376084\n",
            "Epoch 868: train loss = 1588.750934, test loss = 1967.430448\n",
            "Epoch 869: train loss = 1567.811628, test loss = 2023.444408\n",
            "Epoch 870: train loss = 1589.158975, test loss = 1924.800757\n",
            "Epoch 871: train loss = 1546.280306, test loss = 1799.052784\n",
            "Epoch 872: train loss = 1584.832404, test loss = 2376.281696\n",
            "Epoch 873: train loss = 1540.181479, test loss = 1859.663229\n",
            "Epoch 874: train loss = 1537.037707, test loss = 1973.873907\n",
            "Epoch 875: train loss = 1754.884191, test loss = 3651.849355\n",
            "Epoch 876: train loss = 1703.957556, test loss = 1903.052257\n",
            "Epoch 877: train loss = 1548.688752, test loss = 2158.252521\n",
            "Epoch 878: train loss = 1916.208519, test loss = 3675.281519\n",
            "Epoch 879: train loss = 1780.675653, test loss = 1824.054101\n",
            "Epoch 880: train loss = 1581.018680, test loss = 2109.305130\n",
            "Epoch 881: train loss = 1564.351153, test loss = 1812.237939\n",
            "Epoch 882: train loss = 1690.006795, test loss = 2243.082176\n",
            "Epoch 883: train loss = 1565.195913, test loss = 1817.760066\n",
            "Epoch 884: train loss = 1603.804225, test loss = 2190.472854\n",
            "Epoch 885: train loss = 1718.943842, test loss = 2224.395663\n",
            "Epoch 886: train loss = 1614.394818, test loss = 2008.623518\n",
            "Epoch 887: train loss = 1526.673882, test loss = 1907.429132\n",
            "Epoch 888: train loss = 1525.727984, test loss = 1928.122734\n",
            "Epoch 889: train loss = 1732.446803, test loss = 2576.479915\n",
            "Epoch 890: train loss = 1736.667211, test loss = 2210.771637\n",
            "Epoch 891: train loss = 1722.684146, test loss = 2905.806192\n",
            "Epoch 892: train loss = 1615.088740, test loss = 1961.725680\n",
            "Epoch 893: train loss = 1764.910988, test loss = 2593.886516\n",
            "Epoch 894: train loss = 1780.491392, test loss = 2280.136041\n",
            "Epoch 895: train loss = 1592.640186, test loss = 1862.509416\n",
            "Epoch 896: train loss = 1842.239695, test loss = 3400.545860\n",
            "Epoch 897: train loss = 1680.668308, test loss = 1906.173240\n",
            "Epoch 898: train loss = 1539.935577, test loss = 1817.047424\n",
            "Epoch 899: train loss = 1549.018135, test loss = 1801.722175\n",
            "Epoch 900: train loss = 1658.496417, test loss = 2498.111121\n",
            "Epoch 901: train loss = 1635.412742, test loss = 1853.234446\n",
            "Epoch 902: train loss = 1546.980720, test loss = 1841.232153\n",
            "Epoch 903: train loss = 1553.702135, test loss = 2172.738352\n",
            "Epoch 904: train loss = 1554.707133, test loss = 1823.040781\n",
            "Epoch 905: train loss = 1536.031978, test loss = 1831.502181\n",
            "Epoch 906: train loss = 1557.351879, test loss = 1808.016775\n",
            "Epoch 907: train loss = 1537.023909, test loss = 1820.679826\n",
            "Epoch 908: train loss = 1529.262040, test loss = 1859.313261\n",
            "Epoch 909: train loss = 1608.162944, test loss = 2836.435316\n",
            "Epoch 910: train loss = 1654.242153, test loss = 1980.974753\n",
            "Epoch 911: train loss = 1591.895704, test loss = 2057.290615\n",
            "Epoch 912: train loss = 1551.583234, test loss = 1801.827799\n",
            "Epoch 913: train loss = 1610.397704, test loss = 2204.633194\n",
            "Epoch 914: train loss = 1611.861908, test loss = 2132.950413\n",
            "Epoch 915: train loss = 1560.722787, test loss = 1849.911418\n",
            "Epoch 916: train loss = 1646.783535, test loss = 2502.850012\n",
            "Epoch 917: train loss = 1600.786843, test loss = 1842.149584\n",
            "Epoch 918: train loss = 1558.609582, test loss = 1853.451745\n",
            "Epoch 919: train loss = 1583.971381, test loss = 2078.508354\n",
            "Epoch 920: train loss = 1642.012035, test loss = 1955.161794\n",
            "Epoch 921: train loss = 1533.794674, test loss = 1822.409777\n",
            "Epoch 922: train loss = 1564.112624, test loss = 2180.155491\n",
            "Epoch 923: train loss = 1560.038001, test loss = 1947.334272\n",
            "Epoch 924: train loss = 1636.625098, test loss = 2451.171729\n",
            "Epoch 925: train loss = 1600.412839, test loss = 1934.854317\n",
            "Epoch 926: train loss = 1521.450869, test loss = 1845.672030\n",
            "Epoch 927: train loss = 1694.930036, test loss = 2273.504772\n",
            "Epoch 928: train loss = 1596.103456, test loss = 1825.208106\n",
            "Epoch 929: train loss = 1508.679532, test loss = 1840.372650\n",
            "Epoch 930: train loss = 1571.110845, test loss = 1810.221413\n",
            "Epoch 931: train loss = 1759.419618, test loss = 4056.975026\n",
            "Epoch 932: train loss = 1826.464239, test loss = 2888.507308\n",
            "Epoch 933: train loss = 1641.251082, test loss = 1942.637517\n",
            "Epoch 934: train loss = 1936.208743, test loss = 4077.903837\n",
            "Epoch 935: train loss = 1800.231849, test loss = 1879.926469\n",
            "Epoch 936: train loss = 1559.362776, test loss = 1808.287910\n",
            "Epoch 937: train loss = 1574.970370, test loss = 2463.493885\n",
            "Epoch 938: train loss = 1599.768964, test loss = 1791.257483\n",
            "Epoch 939: train loss = 1544.831478, test loss = 1836.041418\n",
            "Epoch 940: train loss = 1702.992342, test loss = 2539.139789\n",
            "Epoch 941: train loss = 1630.936553, test loss = 1928.344219\n",
            "Epoch 942: train loss = 1532.184964, test loss = 1990.478606\n",
            "Epoch 943: train loss = 1581.856936, test loss = 2221.666986\n",
            "Epoch 944: train loss = 1556.628165, test loss = 1902.704467\n",
            "Epoch 945: train loss = 1579.434552, test loss = 2233.315974\n",
            "Epoch 946: train loss = 1573.936442, test loss = 2011.772140\n",
            "Epoch 947: train loss = 1911.292574, test loss = 3201.033185\n",
            "Epoch 948: train loss = 1687.558859, test loss = 1817.294305\n",
            "Epoch 949: train loss = 1802.909925, test loss = 2659.485761\n",
            "Epoch 950: train loss = 1598.640496, test loss = 1768.792352\n",
            "Epoch 951: train loss = 1528.083423, test loss = 1761.346588\n",
            "Epoch 952: train loss = 1557.293375, test loss = 1774.871680\n",
            "Epoch 953: train loss = 1535.339654, test loss = 1860.594876\n",
            "Epoch 954: train loss = 1559.275982, test loss = 1800.605191\n",
            "Epoch 955: train loss = 1554.144652, test loss = 1780.671409\n",
            "Epoch 956: train loss = 1527.271584, test loss = 1790.528152\n",
            "Epoch 957: train loss = 1607.729396, test loss = 1765.923653\n",
            "Epoch 958: train loss = 1560.167226, test loss = 2253.991404\n",
            "Epoch 959: train loss = 1540.856273, test loss = 1789.096244\n",
            "Epoch 960: train loss = 1681.864070, test loss = 2133.240977\n",
            "Epoch 961: train loss = 1662.048170, test loss = 2539.824850\n",
            "Epoch 962: train loss = 1664.498920, test loss = 2109.978938\n",
            "Epoch 963: train loss = 1593.147837, test loss = 1994.850946\n",
            "Epoch 964: train loss = 1531.955497, test loss = 1824.501792\n",
            "Epoch 965: train loss = 1538.110762, test loss = 1844.922964\n",
            "Epoch 966: train loss = 1601.759443, test loss = 2982.275836\n",
            "Epoch 967: train loss = 1606.628864, test loss = 1909.148258\n",
            "Epoch 968: train loss = 1674.787341, test loss = 2169.023731\n",
            "Epoch 969: train loss = 1597.940811, test loss = 2205.928115\n",
            "Epoch 970: train loss = 1560.298496, test loss = 2101.074961\n",
            "Epoch 971: train loss = 1637.220366, test loss = 1930.621784\n",
            "Epoch 972: train loss = 1599.186259, test loss = 1983.867334\n",
            "Epoch 973: train loss = 1526.163249, test loss = 1854.795252\n",
            "Epoch 974: train loss = 1667.218130, test loss = 2824.970574\n",
            "Epoch 975: train loss = 1630.749914, test loss = 2046.544660\n",
            "Epoch 976: train loss = 1551.208091, test loss = 1817.339922\n",
            "Epoch 977: train loss = 1557.439723, test loss = 1826.654731\n",
            "Epoch 978: train loss = 1605.332027, test loss = 2057.701944\n",
            "Epoch 979: train loss = 1659.810030, test loss = 2221.764673\n",
            "Epoch 980: train loss = 1556.119274, test loss = 1924.995763\n",
            "Epoch 981: train loss = 1595.382484, test loss = 1950.932943\n",
            "Epoch 982: train loss = 1563.843438, test loss = 2234.695109\n",
            "Epoch 983: train loss = 1616.375667, test loss = 1941.576932\n",
            "Epoch 984: train loss = 1596.333765, test loss = 1838.407953\n",
            "Epoch 985: train loss = 1544.461740, test loss = 1861.722251\n",
            "Epoch 986: train loss = 1646.309489, test loss = 3496.802590\n",
            "Epoch 987: train loss = 1691.646122, test loss = 1911.929876\n",
            "Epoch 988: train loss = 1655.595487, test loss = 2242.374644\n",
            "Epoch 989: train loss = 1564.287373, test loss = 1814.483928\n",
            "Epoch 990: train loss = 1842.605900, test loss = 3717.430112\n",
            "Epoch 991: train loss = 1717.346558, test loss = 1841.466511\n",
            "Epoch 992: train loss = 1689.579380, test loss = 2405.676371\n",
            "Epoch 993: train loss = 1662.985510, test loss = 2184.472830\n",
            "Epoch 994: train loss = 1605.597264, test loss = 1924.907098\n",
            "Epoch 995: train loss = 1600.891396, test loss = 2492.971072\n",
            "Epoch 996: train loss = 1729.444389, test loss = 2352.583682\n",
            "Epoch 997: train loss = 1647.728031, test loss = 1863.001268\n",
            "Epoch 998: train loss = 1727.409673, test loss = 2354.656065\n",
            "Epoch 999: train loss = 1542.004710, test loss = 1805.241489\n",
            "Epoch 1000: train loss = 1599.433078, test loss = 2226.539227\n"
          ]
        }
      ],
      "source": [
        "## Train the 2 layer neural network (8 nodes followed by 1 node)\n",
        "## using batch training with batch size = 100\n",
        "learning_rate = 1e-03 # learning rate\n",
        "batch_size = 16 # batch size\n",
        "nepochs = 1000 # number of epochs\n",
        "reg_strength = 0.5 # regularization strength\n",
        "# Create empty array to store training losses over each epoch\n",
        "loss_train_epoch = np.empty(nepochs, dtype = np.float64)\n",
        "# Create empty array to store test losses over each epoch\n",
        "loss_test_epoch = np.empty(nepochs, dtype = np.float64)\n",
        "\n",
        "\n",
        "# Neural network architecture\n",
        "\n",
        "dlayer1 = Dense(num_features, 8 , reg_strength) # define dense layer 1\n",
        "alayer1 = ReLU() # ReLU activation layer 1\n",
        "dlayer2 = Dense(8 , 1 , reg_strength) # define dense layer 2\n",
        "\n",
        "# Steps: run over each sample in the batch, calculate loss, gradient of loss,\n",
        "# and update weights.\n",
        "\n",
        "epoch = 0\n",
        "while epoch < nepochs:\n",
        "  batch_indices = generate_batch_indices(num_samples, batch_size)\n",
        "  loss = 0\n",
        "  for b in range(len(batch_indices)):\n",
        "    # Forward propagation for training data\n",
        "    dlayer1.forward(X_train_transformed[:, batch_indices[b]]) # forward prop dense layer 1 with batch feature added\n",
        "    alayer1.forward(dlayer1.output) # forward prop activation layer 1\n",
        "    dlayer2.forward(alayer1.output) # forward prop dense layer 2\n",
        "    # Calculate training data loss\n",
        "    loss += mse(Y_train[batch_indices[b]], dlayer2.output)\n",
        "    # Add the regularization losses\n",
        "    loss += dlayer1.reg_loss + dlayer2.reg_loss\n",
        "\n",
        "    # Backward prop starts here\n",
        "    grad = mse_gradient(Y_train[batch_indices[b]], dlayer2.output)\n",
        "    grad = dlayer2.backward(grad, learning_rate)\n",
        "    grad = alayer1.backward(grad)\n",
        "    grad = dlayer1.backward(grad, learning_rate)\n",
        "  # Calculate the average training loss for the current epoch\n",
        "  loss_train_epoch[epoch] = loss/len(batch_indices)\n",
        "\n",
        "  # Forward propagation for test data\n",
        "  dlayer1.forward(X_test_transformed)\n",
        "  alayer1.forward(dlayer1.output)\n",
        "  dlayer2.forward(alayer1.output)\n",
        "\n",
        "  # Calculate test data loss plus regularization loss\n",
        "  loss_test_epoch[epoch] =  mse(Y_test, dlayer2.output) + dlayer1.reg_loss + dlayer2.reg_loss\n",
        "\n",
        "  print('Epoch %d: train loss = %f, test loss = %f'%(epoch+1, loss_train_epoch[epoch], loss_test_epoch[epoch]))\n",
        "  epoch = epoch + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhXEFASk-Tkv"
      },
      "source": [
        "### Plot training loss vs. epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Iv3k23SlCqGf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "a7a576f0-94a4-4a27-8f8a-9f911b06750f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Loss vs. Epoch for reg. strength 0.01')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot train and test loss as a function of epoch:\n",
        "fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
        "fig.tight_layout(pad = 4.0)\n",
        "ax.plot(loss_train_epoch , 'b', label = 'Train')\n",
        "ax.plot(loss_test_epoch , 'r', label = 'Test')\n",
        "ax.set_xlabel('Epoch', fontsize = 12)\n",
        "ax.set_ylabel('Loss value', fontsize = 12)\n",
        "ax.legend()\n",
        "ax.set_title('Loss vs. Epoch for reg. strength 0.01', fontsize = 14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaLoOOWK-WBj"
      },
      "source": [
        "### Test performance on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "d7AEbmpcKcPY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca76ca03-d94f-48ad-90bd-1d4d9648cbbe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 78.  , 149.28],\n",
              "       [152.  , 106.22],\n",
              "       [200.  , 210.57],\n",
              "       [ 59.  ,  87.2 ],\n",
              "       [311.  , 201.41],\n",
              "       [178.  , 214.79],\n",
              "       [332.  , 253.42],\n",
              "       [132.  , 123.33],\n",
              "       [156.  , 157.72],\n",
              "       [135.  , 163.95],\n",
              "       [220.  , 258.53],\n",
              "       [233.  , 224.94],\n",
              "       [ 91.  , 115.3 ],\n",
              "       [ 51.  ,  92.14],\n",
              "       [195.  , 299.8 ],\n",
              "       [109.  , 246.61],\n",
              "       [217.  , 220.76],\n",
              "       [ 94.  , 140.6 ],\n",
              "       [ 89.  , 129.59],\n",
              "       [111.  , 188.9 ],\n",
              "       [129.  , 219.15],\n",
              "       [181.  , 116.85],\n",
              "       [168.  , 176.53],\n",
              "       [ 97.  , 122.15],\n",
              "       [115.  , 136.79],\n",
              "       [202.  , 251.28],\n",
              "       [ 84.  ,  99.04],\n",
              "       [147.  , 207.69],\n",
              "       [253.  , 164.42],\n",
              "       [144.  , 220.08],\n",
              "       [262.  , 185.86],\n",
              "       [115.  , 188.61],\n",
              "       [ 68.  , 229.43],\n",
              "       [ 65.  , 104.02],\n",
              "       [252.  , 194.09],\n",
              "       [212.  , 222.02],\n",
              "       [142.  , 138.65],\n",
              "       [215.  , 302.48],\n",
              "       [180.  , 207.11],\n",
              "       [163.  , 196.18],\n",
              "       [151.  , 164.62],\n",
              "       [283.  , 205.4 ],\n",
              "       [ 66.  , 126.34],\n",
              "       [ 83.  , 153.34],\n",
              "       [214.  , 169.61],\n",
              "       [189.  , 253.48],\n",
              "       [302.  , 185.41],\n",
              "       [ 93.  , 171.05],\n",
              "       [178.  , 229.48],\n",
              "       [241.  , 223.93],\n",
              "       [ 52.  ,  94.06],\n",
              "       [144.  , 190.51],\n",
              "       [102.  , 128.89],\n",
              "       [200.  , 178.71],\n",
              "       [232.  , 214.3 ],\n",
              "       [ 97.  , 162.68],\n",
              "       [109.  , 211.72],\n",
              "       [ 55.  , 102.02],\n",
              "       [ 63.  ,  89.92],\n",
              "       [ 98.  ,  94.98],\n",
              "       [ 88.  , 111.05],\n",
              "       [233.  , 246.26],\n",
              "       [235.  , 200.31],\n",
              "       [ 97.  , 130.76],\n",
              "       [243.  , 306.42],\n",
              "       [ 59.  , 112.03],\n",
              "       [138.  ,  90.42],\n",
              "       [220.  , 215.18],\n",
              "       [137.  , 249.66],\n",
              "       [ 72.  , 107.64],\n",
              "       [109.  , 226.2 ],\n",
              "       [ 71.  ,  77.84],\n",
              "       [ 74.  ,  98.16],\n",
              "       [219.  , 133.45],\n",
              "       [196.  , 181.38],\n",
              "       [170.  , 137.42],\n",
              "       [199.  , 109.52],\n",
              "       [ 71.  ,  84.6 ],\n",
              "       [155.  , 291.73],\n",
              "       [ 52.  , 199.49],\n",
              "       [ 63.  , 125.67],\n",
              "       [ 88.  , 118.59],\n",
              "       [ 97.  , 189.42],\n",
              "       [100.  , 200.29],\n",
              "       [ 64.  , 140.72],\n",
              "       [107.  , 131.01],\n",
              "       [ 49.  , 147.99],\n",
              "       [ 60.  ,  78.7 ],\n",
              "       [346.  , 231.66]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "dlayer1.forward(X_test_transformed)\n",
        "alayer1.forward(dlayer1.output)\n",
        "dlayer2.forward(alayer1.output)\n",
        "ypred = dlayer2.output.flatten()\n",
        "ytrue = Y_test\n",
        "np.column_stack((ytrue, ypred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}